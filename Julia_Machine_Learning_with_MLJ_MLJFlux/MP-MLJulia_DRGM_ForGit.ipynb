{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Data Science and Machine Learning with Julia Language</font>\n",
    "\n",
    "## <font color='blue'>Machine Learning with Julia Language</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `C:\\MyBkp\\Cursos\\DSA\\DSA_Formacao_Cientista_Dados\\Curso07_Preparacao_Para_Carreira_de_Cientista_de_Dados\\Curso_Bonus_Data_Science_Machine_Learning_com_Linguagem_Julia\\1-MP-MLJulia\\env`\n"
     ]
    }
   ],
   "source": [
    "# Create and instance an env\n",
    "using Pkg\n",
    "Pkg.activate(\"env\")\n",
    "Pkg.instantiate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing MLJ package\n",
    "# Pkg.add(\"MLJ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing MLJ package in this section\n",
    "using MLJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\textbf{Author}: R.A. Fisher   \\textbf{Source}: \\href{https://archive.ics.uci.edu/ml/datasets/Iris}{UCI} - 1936 - Donated by Michael Marshall   \\textbf{Please cite}:   \n",
       "\n",
       "\\textbf{Iris Plants Database}   This is perhaps the best known database to be found in the pattern recognition literature.  Fisher's paper is a classic in the field and is referenced frequently to this day.  (See Duda \\& Hart, for example.)  The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant.  One class is     linearly separable from the other 2; the latter are NOT linearly separable from each other.\n",
       "\n",
       "Predicted attribute: class of iris plant.   This is an exceedingly simple domain.  \n",
       "\n",
       "\\subsubsection{Attribute Information:}\n",
       "\\begin{verbatim}\n",
       "1. sepal length in cm\n",
       "2. sepal width in cm\n",
       "3. petal length in cm\n",
       "4. petal width in cm\n",
       "5. class: \n",
       "   -- Iris Setosa\n",
       "   -- Iris Versicolour\n",
       "   -- Iris Virginica\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "**Author**: R.A. Fisher   **Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Iris) - 1936 - Donated by Michael Marshall   **Please cite**:   \n",
       "\n",
       "**Iris Plants Database**   This is perhaps the best known database to be found in the pattern recognition literature.  Fisher's paper is a classic in the field and is referenced frequently to this day.  (See Duda & Hart, for example.)  The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant.  One class is     linearly separable from the other 2; the latter are NOT linearly separable from each other.\n",
       "\n",
       "Predicted attribute: class of iris plant.   This is an exceedingly simple domain.  \n",
       "\n",
       "### Attribute Information:\n",
       "\n",
       "```\n",
       "1. sepal length in cm\n",
       "2. sepal width in cm\n",
       "3. petal length in cm\n",
       "4. petal width in cm\n",
       "5. class: \n",
       "   -- Iris Setosa\n",
       "   -- Iris Versicolour\n",
       "   -- Iris Virginica\n",
       "```\n"
      ],
      "text/plain": [
       "  \u001b[1mAuthor\u001b[22m: R.A. Fisher \u001b[1mSource\u001b[22m: UCI\n",
       "  (https://archive.ics.uci.edu/ml/datasets/Iris) - 1936 - Donated by Michael\n",
       "  Marshall \u001b[1mPlease cite\u001b[22m:\n",
       "\n",
       "  \u001b[1mIris Plants Database\u001b[22m This is perhaps the best known database to be found in\n",
       "  the pattern recognition literature. Fisher's paper is a classic in the field\n",
       "  and is referenced frequently to this day. (See Duda & Hart, for example.)\n",
       "  The data set contains 3 classes of 50 instances each, where each class\n",
       "  refers to a type of iris plant. One class is linearly separable from the\n",
       "  other 2; the latter are NOT linearly separable from each other.\n",
       "\n",
       "  Predicted attribute: class of iris plant. This is an exceedingly simple\n",
       "  domain.\n",
       "\n",
       "\u001b[1m  Attribute Information:\u001b[22m\n",
       "\u001b[1m  ––––––––––––––––––––––––\u001b[22m\n",
       "\n",
       "\u001b[36m  1. sepal length in cm\u001b[39m\n",
       "\u001b[36m  2. sepal width in cm\u001b[39m\n",
       "\u001b[36m  3. petal length in cm\u001b[39m\n",
       "\u001b[36m  4. petal width in cm\u001b[39m\n",
       "\u001b[36m  5. class: \u001b[39m\n",
       "\u001b[36m     -- Iris Setosa\u001b[39m\n",
       "\u001b[36m     -- Iris Versicolour\u001b[39m\n",
       "\u001b[36m     -- Iris Virginica\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Describing dataset #61\n",
    "OpenML.describe_dataset(61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tables.DictColumnTable with 150 rows, 5 columns, and schema:\n",
       " :sepallength  Float64\n",
       " :sepalwidth   Float64\n",
       " :petallength  Float64\n",
       " :petalwidth   Float64\n",
       " :class        CategoricalArrays.CategoricalValue{String, UInt32}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading dataset #61\n",
    "iris = OpenML.load(61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>150 rows × 5 columns</p><table class=\"data-frame\"><thead><tr><th></th><th>sepallength</th><th>sepalwidth</th><th>petallength</th><th>petalwidth</th><th>class</th></tr><tr><th></th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"CategoricalArrays.CategoricalValue{String, UInt32}\">Cat…</th></tr></thead><tbody><tr><th>1</th><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>2</th><td>4.9</td><td>3.0</td><td>1.4</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>3</th><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>4</th><td>4.6</td><td>3.1</td><td>1.5</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>5</th><td>5.0</td><td>3.6</td><td>1.4</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>6</th><td>5.4</td><td>3.9</td><td>1.7</td><td>0.4</td><td>Iris-setosa</td></tr><tr><th>7</th><td>4.6</td><td>3.4</td><td>1.4</td><td>0.3</td><td>Iris-setosa</td></tr><tr><th>8</th><td>5.0</td><td>3.4</td><td>1.5</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>9</th><td>4.4</td><td>2.9</td><td>1.4</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>10</th><td>4.9</td><td>3.1</td><td>1.5</td><td>0.1</td><td>Iris-setosa</td></tr><tr><th>11</th><td>5.4</td><td>3.7</td><td>1.5</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>12</th><td>4.8</td><td>3.4</td><td>1.6</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>13</th><td>4.8</td><td>3.0</td><td>1.4</td><td>0.1</td><td>Iris-setosa</td></tr><tr><th>14</th><td>4.3</td><td>3.0</td><td>1.1</td><td>0.1</td><td>Iris-setosa</td></tr><tr><th>15</th><td>5.8</td><td>4.0</td><td>1.2</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>16</th><td>5.7</td><td>4.4</td><td>1.5</td><td>0.4</td><td>Iris-setosa</td></tr><tr><th>17</th><td>5.4</td><td>3.9</td><td>1.3</td><td>0.4</td><td>Iris-setosa</td></tr><tr><th>18</th><td>5.1</td><td>3.5</td><td>1.4</td><td>0.3</td><td>Iris-setosa</td></tr><tr><th>19</th><td>5.7</td><td>3.8</td><td>1.7</td><td>0.3</td><td>Iris-setosa</td></tr><tr><th>20</th><td>5.1</td><td>3.8</td><td>1.5</td><td>0.3</td><td>Iris-setosa</td></tr><tr><th>21</th><td>5.4</td><td>3.4</td><td>1.7</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>22</th><td>5.1</td><td>3.7</td><td>1.5</td><td>0.4</td><td>Iris-setosa</td></tr><tr><th>23</th><td>4.6</td><td>3.6</td><td>1.0</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>24</th><td>5.1</td><td>3.3</td><td>1.7</td><td>0.5</td><td>Iris-setosa</td></tr><tr><th>25</th><td>4.8</td><td>3.4</td><td>1.9</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>26</th><td>5.0</td><td>3.0</td><td>1.6</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>27</th><td>5.0</td><td>3.4</td><td>1.6</td><td>0.4</td><td>Iris-setosa</td></tr><tr><th>28</th><td>5.2</td><td>3.5</td><td>1.5</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>29</th><td>5.2</td><td>3.4</td><td>1.4</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>30</th><td>4.7</td><td>3.2</td><td>1.6</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccc}\n",
       "\t& sepallength & sepalwidth & petallength & petalwidth & class\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Cat…\\\\\n",
       "\t\\hline\n",
       "\t1 & 5.1 & 3.5 & 1.4 & 0.2 & Iris-setosa \\\\\n",
       "\t2 & 4.9 & 3.0 & 1.4 & 0.2 & Iris-setosa \\\\\n",
       "\t3 & 4.7 & 3.2 & 1.3 & 0.2 & Iris-setosa \\\\\n",
       "\t4 & 4.6 & 3.1 & 1.5 & 0.2 & Iris-setosa \\\\\n",
       "\t5 & 5.0 & 3.6 & 1.4 & 0.2 & Iris-setosa \\\\\n",
       "\t6 & 5.4 & 3.9 & 1.7 & 0.4 & Iris-setosa \\\\\n",
       "\t7 & 4.6 & 3.4 & 1.4 & 0.3 & Iris-setosa \\\\\n",
       "\t8 & 5.0 & 3.4 & 1.5 & 0.2 & Iris-setosa \\\\\n",
       "\t9 & 4.4 & 2.9 & 1.4 & 0.2 & Iris-setosa \\\\\n",
       "\t10 & 4.9 & 3.1 & 1.5 & 0.1 & Iris-setosa \\\\\n",
       "\t11 & 5.4 & 3.7 & 1.5 & 0.2 & Iris-setosa \\\\\n",
       "\t12 & 4.8 & 3.4 & 1.6 & 0.2 & Iris-setosa \\\\\n",
       "\t13 & 4.8 & 3.0 & 1.4 & 0.1 & Iris-setosa \\\\\n",
       "\t14 & 4.3 & 3.0 & 1.1 & 0.1 & Iris-setosa \\\\\n",
       "\t15 & 5.8 & 4.0 & 1.2 & 0.2 & Iris-setosa \\\\\n",
       "\t16 & 5.7 & 4.4 & 1.5 & 0.4 & Iris-setosa \\\\\n",
       "\t17 & 5.4 & 3.9 & 1.3 & 0.4 & Iris-setosa \\\\\n",
       "\t18 & 5.1 & 3.5 & 1.4 & 0.3 & Iris-setosa \\\\\n",
       "\t19 & 5.7 & 3.8 & 1.7 & 0.3 & Iris-setosa \\\\\n",
       "\t20 & 5.1 & 3.8 & 1.5 & 0.3 & Iris-setosa \\\\\n",
       "\t21 & 5.4 & 3.4 & 1.7 & 0.2 & Iris-setosa \\\\\n",
       "\t22 & 5.1 & 3.7 & 1.5 & 0.4 & Iris-setosa \\\\\n",
       "\t23 & 4.6 & 3.6 & 1.0 & 0.2 & Iris-setosa \\\\\n",
       "\t24 & 5.1 & 3.3 & 1.7 & 0.5 & Iris-setosa \\\\\n",
       "\t25 & 4.8 & 3.4 & 1.9 & 0.2 & Iris-setosa \\\\\n",
       "\t26 & 5.0 & 3.0 & 1.6 & 0.2 & Iris-setosa \\\\\n",
       "\t27 & 5.0 & 3.4 & 1.6 & 0.4 & Iris-setosa \\\\\n",
       "\t28 & 5.2 & 3.5 & 1.5 & 0.2 & Iris-setosa \\\\\n",
       "\t29 & 5.2 & 3.4 & 1.4 & 0.2 & Iris-setosa \\\\\n",
       "\t30 & 4.7 & 3.2 & 1.6 & 0.2 & Iris-setosa \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m150×5 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m sepallength \u001b[0m\u001b[1m sepalwidth \u001b[0m\u001b[1m petallength \u001b[0m\u001b[1m petalwidth \u001b[0m\u001b[1m class          \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Float64     \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Float64     \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Cat…           \u001b[0m\n",
       "─────┼──────────────────────────────────────────────────────────────────\n",
       "   1 │         5.1         3.5          1.4         0.2  Iris-setosa\n",
       "   2 │         4.9         3.0          1.4         0.2  Iris-setosa\n",
       "   3 │         4.7         3.2          1.3         0.2  Iris-setosa\n",
       "   4 │         4.6         3.1          1.5         0.2  Iris-setosa\n",
       "   5 │         5.0         3.6          1.4         0.2  Iris-setosa\n",
       "   6 │         5.4         3.9          1.7         0.4  Iris-setosa\n",
       "   7 │         4.6         3.4          1.4         0.3  Iris-setosa\n",
       "   8 │         5.0         3.4          1.5         0.2  Iris-setosa\n",
       "   9 │         4.4         2.9          1.4         0.2  Iris-setosa\n",
       "  10 │         4.9         3.1          1.5         0.1  Iris-setosa\n",
       "  11 │         5.4         3.7          1.5         0.2  Iris-setosa\n",
       "  ⋮  │      ⋮           ⋮            ⋮           ⋮             ⋮\n",
       " 141 │         6.7         3.1          5.6         2.4  Iris-virginica\n",
       " 142 │         6.9         3.1          5.1         2.3  Iris-virginica\n",
       " 143 │         5.8         2.7          5.1         1.9  Iris-virginica\n",
       " 144 │         6.8         3.2          5.9         2.3  Iris-virginica\n",
       " 145 │         6.7         3.3          5.7         2.5  Iris-virginica\n",
       " 146 │         6.7         3.0          5.2         2.3  Iris-virginica\n",
       " 147 │         6.3         2.5          5.0         1.9  Iris-virginica\n",
       " 148 │         6.5         3.0          5.2         2.0  Iris-virginica\n",
       " 149 │         6.2         3.4          5.4         2.3  Iris-virginica\n",
       " 150 │         5.9         3.0          5.1         1.8  Iris-virginica\n",
       "\u001b[36m                                                        129 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converts dataset into dataframe\n",
    "df = DataFrames.DataFrame(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>4 rows × 5 columns</p><table class=\"data-frame\"><thead><tr><th></th><th>sepallength</th><th>sepalwidth</th><th>petallength</th><th>petalwidth</th><th>class</th></tr><tr><th></th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"CategoricalArrays.CategoricalValue{String, UInt32}\">Cat…</th></tr></thead><tbody><tr><th>1</th><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>2</th><td>4.9</td><td>3.0</td><td>1.4</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>3</th><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>4</th><td>4.6</td><td>3.1</td><td>1.5</td><td>0.2</td><td>Iris-setosa</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccc}\n",
       "\t& sepallength & sepalwidth & petallength & petalwidth & class\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Cat…\\\\\n",
       "\t\\hline\n",
       "\t1 & 5.1 & 3.5 & 1.4 & 0.2 & Iris-setosa \\\\\n",
       "\t2 & 4.9 & 3.0 & 1.4 & 0.2 & Iris-setosa \\\\\n",
       "\t3 & 4.7 & 3.2 & 1.3 & 0.2 & Iris-setosa \\\\\n",
       "\t4 & 4.6 & 3.1 & 1.5 & 0.2 & Iris-setosa \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m4×5 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m sepallength \u001b[0m\u001b[1m sepalwidth \u001b[0m\u001b[1m petallength \u001b[0m\u001b[1m petalwidth \u001b[0m\u001b[1m class       \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Float64     \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Float64     \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Cat…        \u001b[0m\n",
       "─────┼───────────────────────────────────────────────────────────────\n",
       "   1 │         5.1         3.5          1.4         0.2  Iris-setosa\n",
       "   2 │         4.9         3.0          1.4         0.2  Iris-setosa\n",
       "   3 │         4.7         3.2          1.3         0.2  Iris-setosa\n",
       "   4 │         4.6         3.1          1.5         0.2  Iris-setosa"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualizing a little sample of the dataset\n",
    "first(df, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌─────────────┬───────────────┬──────────────────────────────────┐\n",
       "│\u001b[22m names       \u001b[0m│\u001b[22m scitypes      \u001b[0m│\u001b[22m types                            \u001b[0m│\n",
       "├─────────────┼───────────────┼──────────────────────────────────┤\n",
       "│ sepallength │ Continuous    │ Float64                          │\n",
       "│ sepalwidth  │ Continuous    │ Float64                          │\n",
       "│ petallength │ Continuous    │ Float64                          │\n",
       "│ petalwidth  │ Continuous    │ Float64                          │\n",
       "│ class       │ Multiclass{3} │ CategoricalValue{String, UInt32} │\n",
       "└─────────────┴───────────────┴──────────────────────────────────┘\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataframe Schema (metadata)\n",
    "schema(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(CategoricalArrays.CategoricalValue{String, UInt32}[\"Iris-virginica\", \"Iris-versicolor\", \"Iris-virginica\", \"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\", \"Iris-setosa\", \"Iris-versicolor\", \"Iris-setosa\", \"Iris-virginica\"  …  \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-versicolor\", \"Iris-virginica\", \"Iris-setosa\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-setosa\", \"Iris-setosa\"], \u001b[1m150×4 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m sepallength \u001b[0m\u001b[1m sepalwidth \u001b[0m\u001b[1m petallength \u001b[0m\u001b[1m petalwidth \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Float64     \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Float64     \u001b[0m\u001b[90m Float64    \u001b[0m\n",
       "─────┼──────────────────────────────────────────────────\n",
       "   1 │         6.7         3.3          5.7         2.1\n",
       "   2 │         5.7         2.8          4.1         1.3\n",
       "   3 │         7.2         3.0          5.8         1.6\n",
       "   4 │         4.4         2.9          1.4         0.2\n",
       "   5 │         5.6         2.5          3.9         1.1\n",
       "   6 │         6.5         3.0          5.2         2.0\n",
       "   7 │         4.4         3.0          1.3         0.2\n",
       "   8 │         6.1         2.9          4.7         1.4\n",
       "   9 │         5.4         3.9          1.7         0.4\n",
       "  10 │         4.9         2.5          4.5         1.7\n",
       "  11 │         6.3         2.5          4.9         1.5\n",
       "  ⋮  │      ⋮           ⋮            ⋮           ⋮\n",
       " 141 │         6.4         2.7          5.3         1.9\n",
       " 142 │         6.8         3.2          5.9         2.3\n",
       " 143 │         6.9         3.1          5.4         2.1\n",
       " 144 │         6.1         2.8          4.0         1.3\n",
       " 145 │         6.7         2.5          5.8         1.8\n",
       " 146 │         5.0         3.5          1.3         0.3\n",
       " 147 │         7.6         3.0          6.6         2.1\n",
       " 148 │         6.3         2.5          5.0         1.9\n",
       " 149 │         5.1         3.8          1.6         0.2\n",
       " 150 │         5.0         3.6          1.4         0.2\n",
       "\u001b[36m                                        129 rows omitted\u001b[0m)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract x and y from the dataframe\n",
    "# x = predictable variables\n",
    "# y = target variable (class)\n",
    "y, X = unpack(df, ==(:class), rng = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AbstractVector{Multiclass{3}} (alias for AbstractArray{Multiclass{3}, 1})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y variable schema\n",
    "scitype(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "unpack(table, f1, f2, ... fk;\n",
       "       wrap_singles=false,\n",
       "       shuffle=false,\n",
       "       rng::Union{AbstractRNG,Int,Nothing}=nothing,\n",
       "       coerce_options...)\n",
       "\\end{verbatim}\n",
       "Horizontally split any Tables.jl compatible \\texttt{table} into smaller tables or vectors by making column selections determined by the predicates \\texttt{f1}, \\texttt{f2}, ..., \\texttt{fk}. Selection from the column names is without replacement. A \\emph{predicate} is any object \\texttt{f} such that \\texttt{f(name)} is \\texttt{true} or \\texttt{false} for each column \\texttt{name::Symbol} of \\texttt{table}.\n",
       "\n",
       "Returns a tuple of tables/vectors with length one greater than the number of supplied predicates, with the last component including all previously unselected columns.\n",
       "\n",
       "\\begin{verbatim}\n",
       "julia> table = DataFrame(x=[1,2], y=['a', 'b'], z=[10.0, 20.0], w=[\"A\", \"B\"])\n",
       "2×4 DataFrame\n",
       " Row │ x      y     z        w\n",
       "     │ Int64  Char  Float64  String\n",
       "─────┼──────────────────────────────\n",
       "   1 │     1  a        10.0  A\n",
       "   2 │     2  b        20.0  B\n",
       "\n",
       "Z, XY, W = unpack(table, ==(:z), !=(:w))\n",
       "julia> Z\n",
       "2-element Vector{Float64}:\n",
       " 10.0\n",
       " 20.0\n",
       "\n",
       "julia> XY\n",
       "2×2 DataFrame\n",
       " Row │ x      y\n",
       "     │ Int64  Char\n",
       "─────┼─────────────\n",
       "   1 │     1  a\n",
       "   2 │     2  b\n",
       "\n",
       "julia> W  # the column(s) left over\n",
       "2-element Vector{String}:\n",
       " \"A\"\n",
       " \"B\"\n",
       "\\end{verbatim}\n",
       "Whenever a returned table contains a single column, it is converted to a vector unless \\texttt{wrap\\_singles=true}.\n",
       "\n",
       "If \\texttt{coerce\\_options} are specified then \\texttt{table} is first replaced with \\texttt{coerce(table, coerce\\_options)}. See \\href{@ref}{\\texttt{ScientificTypes.coerce}} for details.\n",
       "\n",
       "If \\texttt{shuffle=true} then the rows of \\texttt{table} are first shuffled, using the global RNG, unless \\texttt{rng} is specified; if \\texttt{rng} is an integer, it specifies the seed of an automatically generated Mersenne twister. If \\texttt{rng} is specified then \\texttt{shuffle=true} is implicit.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "unpack(table, f1, f2, ... fk;\n",
       "       wrap_singles=false,\n",
       "       shuffle=false,\n",
       "       rng::Union{AbstractRNG,Int,Nothing}=nothing,\n",
       "       coerce_options...)\n",
       "```\n",
       "\n",
       "Horizontally split any Tables.jl compatible `table` into smaller tables or vectors by making column selections determined by the predicates `f1`, `f2`, ..., `fk`. Selection from the column names is without replacement. A *predicate* is any object `f` such that `f(name)` is `true` or `false` for each column `name::Symbol` of `table`.\n",
       "\n",
       "Returns a tuple of tables/vectors with length one greater than the number of supplied predicates, with the last component including all previously unselected columns.\n",
       "\n",
       "```\n",
       "julia> table = DataFrame(x=[1,2], y=['a', 'b'], z=[10.0, 20.0], w=[\"A\", \"B\"])\n",
       "2×4 DataFrame\n",
       " Row │ x      y     z        w\n",
       "     │ Int64  Char  Float64  String\n",
       "─────┼──────────────────────────────\n",
       "   1 │     1  a        10.0  A\n",
       "   2 │     2  b        20.0  B\n",
       "\n",
       "Z, XY, W = unpack(table, ==(:z), !=(:w))\n",
       "julia> Z\n",
       "2-element Vector{Float64}:\n",
       " 10.0\n",
       " 20.0\n",
       "\n",
       "julia> XY\n",
       "2×2 DataFrame\n",
       " Row │ x      y\n",
       "     │ Int64  Char\n",
       "─────┼─────────────\n",
       "   1 │     1  a\n",
       "   2 │     2  b\n",
       "\n",
       "julia> W  # the column(s) left over\n",
       "2-element Vector{String}:\n",
       " \"A\"\n",
       " \"B\"\n",
       "```\n",
       "\n",
       "Whenever a returned table contains a single column, it is converted to a vector unless `wrap_singles=true`.\n",
       "\n",
       "If `coerce_options` are specified then `table` is first replaced with `coerce(table, coerce_options)`. See [`ScientificTypes.coerce`](@ref) for details.\n",
       "\n",
       "If `shuffle=true` then the rows of `table` are first shuffled, using the global RNG, unless `rng` is specified; if `rng` is an integer, it specifies the seed of an automatically generated Mersenne twister. If `rng` is specified then `shuffle=true` is implicit.\n"
      ],
      "text/plain": [
       "\u001b[36m  unpack(table, f1, f2, ... fk;\u001b[39m\n",
       "\u001b[36m         wrap_singles=false,\u001b[39m\n",
       "\u001b[36m         shuffle=false,\u001b[39m\n",
       "\u001b[36m         rng::Union{AbstractRNG,Int,Nothing}=nothing,\u001b[39m\n",
       "\u001b[36m         coerce_options...)\u001b[39m\n",
       "\n",
       "  Horizontally split any Tables.jl compatible \u001b[36mtable\u001b[39m into smaller tables or\n",
       "  vectors by making column selections determined by the predicates \u001b[36mf1\u001b[39m, \u001b[36mf2\u001b[39m,\n",
       "  ..., \u001b[36mfk\u001b[39m. Selection from the column names is without replacement. A \u001b[4mpredicate\u001b[24m\n",
       "  is any object \u001b[36mf\u001b[39m such that \u001b[36mf(name)\u001b[39m is \u001b[36mtrue\u001b[39m or \u001b[36mfalse\u001b[39m for each column\n",
       "  \u001b[36mname::Symbol\u001b[39m of \u001b[36mtable\u001b[39m.\n",
       "\n",
       "  Returns a tuple of tables/vectors with length one greater than the number of\n",
       "  supplied predicates, with the last component including all previously\n",
       "  unselected columns.\n",
       "\n",
       "\u001b[36m  julia> table = DataFrame(x=[1,2], y=['a', 'b'], z=[10.0, 20.0], w=[\"A\", \"B\"])\u001b[39m\n",
       "\u001b[36m  2×4 DataFrame\u001b[39m\n",
       "\u001b[36m   Row │ x      y     z        w\u001b[39m\n",
       "\u001b[36m       │ Int64  Char  Float64  String\u001b[39m\n",
       "\u001b[36m  ─────┼──────────────────────────────\u001b[39m\n",
       "\u001b[36m     1 │     1  a        10.0  A\u001b[39m\n",
       "\u001b[36m     2 │     2  b        20.0  B\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  Z, XY, W = unpack(table, ==(:z), !=(:w))\u001b[39m\n",
       "\u001b[36m  julia> Z\u001b[39m\n",
       "\u001b[36m  2-element Vector{Float64}:\u001b[39m\n",
       "\u001b[36m   10.0\u001b[39m\n",
       "\u001b[36m   20.0\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> XY\u001b[39m\n",
       "\u001b[36m  2×2 DataFrame\u001b[39m\n",
       "\u001b[36m   Row │ x      y\u001b[39m\n",
       "\u001b[36m       │ Int64  Char\u001b[39m\n",
       "\u001b[36m  ─────┼─────────────\u001b[39m\n",
       "\u001b[36m     1 │     1  a\u001b[39m\n",
       "\u001b[36m     2 │     2  b\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> W  # the column(s) left over\u001b[39m\n",
       "\u001b[36m  2-element Vector{String}:\u001b[39m\n",
       "\u001b[36m   \"A\"\u001b[39m\n",
       "\u001b[36m   \"B\"\u001b[39m\n",
       "\n",
       "  Whenever a returned table contains a single column, it is converted to a\n",
       "  vector unless \u001b[36mwrap_singles=true\u001b[39m.\n",
       "\n",
       "  If \u001b[36mcoerce_options\u001b[39m are specified then \u001b[36mtable\u001b[39m is first replaced with\n",
       "  \u001b[36mcoerce(table, coerce_options)\u001b[39m. See \u001b[36mScientificTypes.coerce\u001b[39m for details.\n",
       "\n",
       "  If \u001b[36mshuffle=true\u001b[39m then the rows of \u001b[36mtable\u001b[39m are first shuffled, using the global\n",
       "  RNG, unless \u001b[36mrng\u001b[39m is specified; if \u001b[36mrng\u001b[39m is an integer, it specifies the seed of\n",
       "  an automatically generated Mersenne twister. If \u001b[36mrng\u001b[39m is specified then\n",
       "  \u001b[36mshuffle=true\u001b[39m is implicit."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function documentation\n",
    "@doc unpack\n",
    "\n",
    "# ou ?unpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :deep_properties, :docstring, :fit_data_scitype, :human_name, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :reporting_operations, :reports_feature_importances, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :transform_scitype, :input_scitype, :target_scitype, :output_scitype)}}:\n",
       " (name = ABODDetector, package_name = OutlierDetectionNeighbors, ... )\n",
       " (name = ABODDetector, package_name = OutlierDetectionPython, ... )\n",
       " (name = AEDetector, package_name = OutlierDetectionNetworks, ... )\n",
       " (name = ARDRegressor, package_name = ScikitLearn, ... )\n",
       " (name = AdaBoostClassifier, package_name = ScikitLearn, ... )\n",
       " (name = AdaBoostRegressor, package_name = ScikitLearn, ... )\n",
       " (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n",
       " (name = AffinityPropagation, package_name = ScikitLearn, ... )\n",
       " (name = AgglomerativeClustering, package_name = ScikitLearn, ... )\n",
       " (name = BM25Transformer, package_name = MLJText, ... )\n",
       " (name = BaggingClassifier, package_name = ScikitLearn, ... )\n",
       " (name = BaggingRegressor, package_name = ScikitLearn, ... )\n",
       " (name = BayesianLDA, package_name = MultivariateStats, ... )\n",
       " ⋮\n",
       " (name = SubspaceLDA, package_name = MultivariateStats, ... )\n",
       " (name = TSVDTransformer, package_name = TSVD, ... )\n",
       " (name = TfidfTransformer, package_name = MLJText, ... )\n",
       " (name = TheilSenRegressor, package_name = ScikitLearn, ... )\n",
       " (name = UnivariateBoxCoxTransformer, package_name = MLJModels, ... )\n",
       " (name = UnivariateDiscretizer, package_name = MLJModels, ... )\n",
       " (name = UnivariateFillImputer, package_name = MLJModels, ... )\n",
       " (name = UnivariateStandardizer, package_name = MLJModels, ... )\n",
       " (name = UnivariateTimeTypeToContinuous, package_name = MLJModels, ... )\n",
       " (name = XGBoostClassifier, package_name = XGBoost, ... )\n",
       " (name = XGBoostCount, package_name = XGBoost, ... )\n",
       " (name = XGBoostRegressor, package_name = XGBoost, ... )"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Searching for all ML models available\n",
    "all_models = models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :deep_properties, :docstring, :fit_data_scitype, :human_name, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :reporting_operations, :reports_feature_importances, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :transform_scitype, :input_scitype, :target_scitype, :output_scitype)}}:\n",
       " (name = LinearBinaryClassifier, package_name = GLM, ... )\n",
       " (name = LinearCountRegressor, package_name = GLM, ... )\n",
       " (name = LinearRegressor, package_name = GLM, ... )\n",
       " (name = LinearRegressor, package_name = MLJLinearModels, ... )\n",
       " (name = LinearRegressor, package_name = MultivariateStats, ... )\n",
       " (name = LinearRegressor, package_name = ScikitLearn, ... )\n",
       " (name = MultitargetLinearRegressor, package_name = MultivariateStats, ... )\n",
       " (name = MultitargetRidgeRegressor, package_name = MultivariateStats, ... )\n",
       " (name = RidgeRegressor, package_name = MultivariateStats, ... )\n",
       " (name = SVMLinearRegressor, package_name = ScikitLearn, ... )"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Searching for every ML Linear Regressor models available\n",
    "some_models = models(\"LinearRegressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(name = \"LinearRegressor\",\n",
       " package_name = \"GLM\",\n",
       " is_supervised = true,\n",
       " abstract_type = Probabilistic,\n",
       " deep_properties = (),\n",
       " docstring = \"```\\nLinearRegressor\\n```\\n\\nA model type for construc...\",\n",
       " fit_data_scitype =\n",
       "     Union{Tuple{Table{<:AbstractVector{<:Continuous}}, AbstractVector{Continuous}}, Tuple{Table{<:AbstractVector{<:Continuous}}, AbstractVector{Continuous}, AbstractVector{<:Union{Continuous, Count}}}},\n",
       " human_name = \"linear regressor\",\n",
       " hyperparameter_ranges = (nothing, nothing, nothing, nothing),\n",
       " hyperparameter_types = (\"Bool\",\n",
       "                         \"Bool\",\n",
       "                         \"Union{Nothing, Symbol}\",\n",
       "                         \"Union{Nothing, AbstractVector{Symbol}}\"),\n",
       " hyperparameters = (:fit_intercept, :dropcollinear, :offsetcol, :report_keys),\n",
       " implemented_methods =\n",
       "     [:clean!, :fitted_params, :predict, :predict_mean, :fit],\n",
       " inverse_transform_scitype = Unknown,\n",
       " is_pure_julia = true,\n",
       " is_wrapper = false,\n",
       " iteration_parameter = nothing,\n",
       " load_path = \"MLJGLMInterface.LinearRegressor\",\n",
       " package_license = \"MIT\",\n",
       " package_url = \"https://github.com/JuliaStats/GLM.jl\",\n",
       " package_uuid = \"38e38edf-8417-5370-95a0-9cbb8c7f171a\",\n",
       " predict_scitype = AbstractVector{ScientificTypesBase.Density{Continuous}},\n",
       " prediction_type = :probabilistic,\n",
       " reporting_operations = (),\n",
       " reports_feature_importances = false,\n",
       " supports_class_weights = false,\n",
       " supports_online = false,\n",
       " supports_training_losses = false,\n",
       " supports_weights = true,\n",
       " transform_scitype = Unknown,\n",
       " input_scitype = Table{<:AbstractVector{<:Continuous}},\n",
       " target_scitype = AbstractVector{Continuous},\n",
       " output_scitype = Unknown)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualizing the details of the model\n",
    "meta = some_models[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AbstractVector{Continuous} (alias for AbstractArray{Continuous, 1})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Type of the target variable of the previous model\n",
    "targetscitype = meta.target_scitype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifying if the previous model can be used with the target variable\n",
    "# We can't because we have a regressor model and we have categorical target variable\n",
    "scitype(y) <: targetscitype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filter_julia_classifiers (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to filter every classification models\n",
    "filter_julia_classifiers(meta) = AbstractVector{Finite} <: meta.target_scitype && meta.is_pure_julia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :deep_properties, :docstring, :fit_data_scitype, :human_name, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :reporting_operations, :reports_feature_importances, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :transform_scitype, :input_scitype, :target_scitype, :output_scitype)}}:\n",
       " (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n",
       " (name = BayesianLDA, package_name = MultivariateStats, ... )\n",
       " (name = BayesianSubspaceLDA, package_name = MultivariateStats, ... )\n",
       " (name = ConstantClassifier, package_name = MLJModels, ... )\n",
       " (name = DecisionTreeClassifier, package_name = BetaML, ... )\n",
       " (name = DecisionTreeClassifier, package_name = DecisionTree, ... )\n",
       " (name = DeterministicConstantClassifier, package_name = MLJModels, ... )\n",
       " (name = EvoTreeClassifier, package_name = EvoTrees, ... )\n",
       " (name = GaussianNBClassifier, package_name = NaiveBayes, ... )\n",
       " (name = KNNClassifier, package_name = NearestNeighborModels, ... )\n",
       " (name = KernelPerceptronClassifier, package_name = BetaML, ... )\n",
       " (name = LDA, package_name = MultivariateStats, ... )\n",
       " (name = LogisticClassifier, package_name = MLJLinearModels, ... )\n",
       " (name = MultinomialClassifier, package_name = MLJLinearModels, ... )\n",
       " (name = MultinomialNBClassifier, package_name = NaiveBayes, ... )\n",
       " (name = NeuralNetworkClassifier, package_name = MLJFlux, ... )\n",
       " (name = OneRuleClassifier, package_name = OneRule, ... )\n",
       " (name = PegasosClassifier, package_name = BetaML, ... )\n",
       " (name = PerceptronClassifier, package_name = BetaML, ... )\n",
       " (name = RandomForestClassifier, package_name = BetaML, ... )\n",
       " (name = RandomForestClassifier, package_name = DecisionTree, ... )\n",
       " (name = SubspaceLDA, package_name = MultivariateStats, ... )"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter classification models\n",
    "models(filter_julia_classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :deep_properties, :docstring, :fit_data_scitype, :human_name, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :reporting_operations, :reports_feature_importances, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :transform_scitype, :input_scitype, :target_scitype, :output_scitype)}}:\n",
       " (name = AdaBoostClassifier, package_name = ScikitLearn, ... )\n",
       " (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n",
       " (name = BaggingClassifier, package_name = ScikitLearn, ... )\n",
       " (name = BayesianLDA, package_name = MultivariateStats, ... )\n",
       " (name = BayesianLDA, package_name = ScikitLearn, ... )\n",
       " (name = BayesianQDA, package_name = ScikitLearn, ... )\n",
       " (name = BayesianSubspaceLDA, package_name = MultivariateStats, ... )\n",
       " (name = ConstantClassifier, package_name = MLJModels, ... )\n",
       " (name = DecisionTreeClassifier, package_name = BetaML, ... )\n",
       " (name = DecisionTreeClassifier, package_name = DecisionTree, ... )\n",
       " (name = DeterministicConstantClassifier, package_name = MLJModels, ... )\n",
       " (name = DummyClassifier, package_name = ScikitLearn, ... )\n",
       " (name = EvoTreeClassifier, package_name = EvoTrees, ... )\n",
       " ⋮\n",
       " (name = RandomForestClassifier, package_name = BetaML, ... )\n",
       " (name = RandomForestClassifier, package_name = DecisionTree, ... )\n",
       " (name = RandomForestClassifier, package_name = ScikitLearn, ... )\n",
       " (name = RidgeCVClassifier, package_name = ScikitLearn, ... )\n",
       " (name = RidgeClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SGDClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVC, package_name = LIBSVM, ... )\n",
       " (name = SVMClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVMLinearClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVMNuClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SubspaceLDA, package_name = MultivariateStats, ... )\n",
       " (name = XGBoostClassifier, package_name = XGBoost, ... )"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which models can be used with x and y variables?\n",
    "models(matching(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `C:\\Users\\drena\\.julia\\registries\\General.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m NNlib ──────────────── v0.8.9\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ShowCases ──────────── v0.1.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m BFloat16s ──────────── v0.2.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m TimerOutputs ───────── v0.5.21\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m NNlibCUDA ──────────── v0.2.4\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Optimisers ─────────── v0.2.9\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ZygoteRules ────────── v0.2.2\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m GPUArrays ──────────── v8.5.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Static ─────────────── v0.4.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m RealDot ────────────── v0.1.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m MLJFlux ────────────── v0.2.8\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m MLUtils ────────────── v0.2.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m RandomNumbers ──────── v1.5.3\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m LLVM ───────────────── v4.14.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m IRTools ────────────── v0.4.6\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m IfElse ─────────────── v0.1.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ProgressLogging ────── v0.1.4\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Metalhead ──────────── v0.7.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Random123 ──────────── v1.6.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Zygote ─────────────── v0.6.45\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ChainRules ─────────── v1.44.4\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ExprTools ──────────── v0.1.8\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m CommonSubexpressions ─ v0.3.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m DiffRules ──────────── v1.11.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m DiffResults ────────── v1.0.3\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m AbstractFFTs ───────── v1.2.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m LLVMExtra_jll ──────── v0.0.16+0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ArrayInterface ─────── v3.1.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Functors ───────────── v0.2.8\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Flux ───────────────── v0.13.4\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m PartialFunctions ───── v1.1.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m GPUArraysCore ──────── v0.1.2\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m BSON ───────────────── v0.3.5\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ForwardDiff ────────── v0.10.32\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m GPUCompiler ────────── v0.16.3\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m NeuralAttentionlib ─── v0.0.5\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m CUDA ───────────────── v3.12.0\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `C:\\MyBkp\\Cursos\\DSA\\DSA_Formacao_Cientista_Dados\\Curso07_Preparacao_Para_Carreira_de_Cientista_de_Dados\\Curso_Bonus_Data_Science_Machine_Learning_com_Linguagem_Julia\\1-MP-MLJulia\\env\\Project.toml`\n",
      " \u001b[90m [094fc8d1] \u001b[39m\u001b[92m+ MLJFlux v0.2.8\u001b[39m\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `C:\\MyBkp\\Cursos\\DSA\\DSA_Formacao_Cientista_Dados\\Curso07_Preparacao_Para_Carreira_de_Cientista_de_Dados\\Curso_Bonus_Data_Science_Machine_Learning_com_Linguagem_Julia\\1-MP-MLJulia\\env\\Manifest.toml`\n",
      " \u001b[90m [621f4979] \u001b[39m\u001b[92m+ AbstractFFTs v1.2.1\u001b[39m\n",
      " \u001b[90m [4fba245c] \u001b[39m\u001b[92m+ ArrayInterface v3.1.1\u001b[39m\n",
      " \u001b[90m [ab4f0b2a] \u001b[39m\u001b[92m+ BFloat16s v0.2.0\u001b[39m\n",
      " \u001b[90m [fbb218c0] \u001b[39m\u001b[92m+ BSON v0.3.5\u001b[39m\n",
      " \u001b[90m [052768ef] \u001b[39m\u001b[92m+ CUDA v3.12.0\u001b[39m\n",
      " \u001b[90m [082447d4] \u001b[39m\u001b[92m+ ChainRules v1.44.4\u001b[39m\n",
      " \u001b[90m [bbf7d656] \u001b[39m\u001b[92m+ CommonSubexpressions v0.3.0\u001b[39m\n",
      " \u001b[90m [163ba53b] \u001b[39m\u001b[92m+ DiffResults v1.0.3\u001b[39m\n",
      " \u001b[90m [b552c78f] \u001b[39m\u001b[92m+ DiffRules v1.11.1\u001b[39m\n",
      " \u001b[90m [e2ba6199] \u001b[39m\u001b[92m+ ExprTools v0.1.8\u001b[39m\n",
      " \u001b[90m [587475ba] \u001b[39m\u001b[92m+ Flux v0.13.4\u001b[39m\n",
      " \u001b[90m [f6369f11] \u001b[39m\u001b[92m+ ForwardDiff v0.10.32\u001b[39m\n",
      " \u001b[90m [d9f16b24] \u001b[39m\u001b[92m+ Functors v0.2.8\u001b[39m\n",
      " \u001b[90m [0c68f7d7] \u001b[39m\u001b[92m+ GPUArrays v8.5.0\u001b[39m\n",
      " \u001b[90m [46192b85] \u001b[39m\u001b[92m+ GPUArraysCore v0.1.2\u001b[39m\n",
      " \u001b[90m [61eb1bfa] \u001b[39m\u001b[92m+ GPUCompiler v0.16.3\u001b[39m\n",
      " \u001b[90m [7869d1d1] \u001b[39m\u001b[92m+ IRTools v0.4.6\u001b[39m\n",
      " \u001b[90m [615f187c] \u001b[39m\u001b[92m+ IfElse v0.1.1\u001b[39m\n",
      " \u001b[90m [929cbde3] \u001b[39m\u001b[92m+ LLVM v4.14.0\u001b[39m\n",
      " \u001b[90m [094fc8d1] \u001b[39m\u001b[92m+ MLJFlux v0.2.8\u001b[39m\n",
      " \u001b[90m [f1d291b0] \u001b[39m\u001b[92m+ MLUtils v0.2.1\u001b[39m\n",
      " \u001b[90m [1914dd2f] \u001b[39m\u001b[92m+ MacroTools v0.5.9\u001b[39m\n",
      " \u001b[90m [dbeba491] \u001b[39m\u001b[92m+ Metalhead v0.7.1\u001b[39m\n",
      " \u001b[90m [872c559c] \u001b[39m\u001b[92m+ NNlib v0.8.9\u001b[39m\n",
      " \u001b[90m [a00861dc] \u001b[39m\u001b[92m+ NNlibCUDA v0.2.4\u001b[39m\n",
      " \u001b[90m [12afc1b8] \u001b[39m\u001b[92m+ NeuralAttentionlib v0.0.5\u001b[39m\n",
      " \u001b[90m [3bd65402] \u001b[39m\u001b[92m+ Optimisers v0.2.9\u001b[39m\n",
      " \u001b[90m [570af359] \u001b[39m\u001b[92m+ PartialFunctions v1.1.1\u001b[39m\n",
      " \u001b[90m [33c8b6b6] \u001b[39m\u001b[92m+ ProgressLogging v0.1.4\u001b[39m\n",
      " \u001b[90m [74087812] \u001b[39m\u001b[92m+ Random123 v1.6.0\u001b[39m\n",
      " \u001b[90m [e6cf234a] \u001b[39m\u001b[92m+ RandomNumbers v1.5.3\u001b[39m\n",
      " \u001b[90m [c1ae055f] \u001b[39m\u001b[92m+ RealDot v0.1.0\u001b[39m\n",
      " \u001b[90m [605ecd9f] \u001b[39m\u001b[92m+ ShowCases v0.1.0\u001b[39m\n",
      " \u001b[90m [aedffcd0] \u001b[39m\u001b[92m+ Static v0.4.1\u001b[39m\n",
      " \u001b[90m [a759f4b9] \u001b[39m\u001b[92m+ TimerOutputs v0.5.21\u001b[39m\n",
      " \u001b[90m [e88e6eb3] \u001b[39m\u001b[92m+ Zygote v0.6.45\u001b[39m\n",
      " \u001b[90m [700de1a5] \u001b[39m\u001b[92m+ ZygoteRules v0.2.2\u001b[39m\n",
      " \u001b[90m [dad2f222] \u001b[39m\u001b[92m+ LLVMExtra_jll v0.0.16+0\u001b[39m\n",
      " \u001b[90m [4af54fe1] \u001b[39m\u001b[92m+ LazyArtifacts\u001b[39m\n",
      "\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mRealDot\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mIfElse\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mPartialFunctions\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mExprTools\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mBSON\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mShowCases\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mBFloat16s\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mProgressLogging\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mFunctors\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mGPUArraysCore\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mRandomNumbers\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mAbstractFFTs\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mCommonSubexpressions\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mZygoteRules\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mDiffRules\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mIRTools\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mDiffResults\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mLLVMExtra_jll\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mStatic\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mArrayInterface\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mOptimisers\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mTimerOutputs\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMLUtils\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mNNlib\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mRandom123\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mForwardDiff\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mLLVM\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mChainRules\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mGPUArrays\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mGPUCompiler\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mZygote\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mCUDA\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mNNlibCUDA\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mNeuralAttentionlib\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mFlux\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMetalhead\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39mMLJFlux\n",
      "  37 dependencies successfully precompiled in 85 seconds (107 already precompiled)\n"
     ]
    }
   ],
   "source": [
    "# Installing MLJFlux package\n",
    "Pkg.add(\"MLJFlux\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: For silent loading, specify `verbosity=0`. \n",
      "└ @ Main C:\\Users\\drena\\.julia\\packages\\MLJModels\\hAzAn\\src\\loading.jl:159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import MLJFlux ✔\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLJFlux.NeuralNetworkClassifier"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading Neural network algorithm\n",
    "NeuralNetworkClassifier = @load NeuralNetworkClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetworkClassifier(\n",
       "  builder = Short(\n",
       "        n_hidden = 0, \n",
       "        dropout = 0.5, \n",
       "        σ = NNlib.σ), \n",
       "  finaliser = NNlib.softmax, \n",
       "  optimiser = Flux.Optimise.Adam(0.001, (0.9, 0.999), 1.0e-8, IdDict{Any, Any}()), \n",
       "  loss = Flux.Losses.crossentropy, \n",
       "  epochs = 10, \n",
       "  batch_size = 1, \n",
       "  lambda = 0.0, \n",
       "  alpha = 0.0, \n",
       "  rng = Random._GLOBAL_RNG(), \n",
       "  optimiser_changes_trigger_retraining = false, \n",
       "  acceleration = CPU1{Nothing}(nothing))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates the model\n",
    "model = NeuralNetworkClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: both Losses and NNlib export \"ctc_loss\"; uses of it in module Flux must be qualified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(name = \"NeuralNetworkClassifier\",\n",
       " package_name = \"MLJFlux\",\n",
       " is_supervised = true,\n",
       " abstract_type = Probabilistic,\n",
       " deep_properties = (:optimiser, :builder),\n",
       " docstring = \"```\\nNeuralNetworkClassifier\\n```\\n\\nA model type for ...\",\n",
       " fit_data_scitype =\n",
       "     Tuple{Table{<:AbstractVector{<:Continuous}}, AbstractVector{<:Finite}},\n",
       " human_name = \"neural network classifier\",\n",
       " hyperparameter_ranges = (nothing,\n",
       "                          nothing,\n",
       "                          nothing,\n",
       "                          nothing,\n",
       "                          nothing,\n",
       "                          nothing,\n",
       "                          nothing,\n",
       "                          nothing,\n",
       "                          nothing,\n",
       "                          nothing,\n",
       "                          nothing),\n",
       " hyperparameter_types = (\"MLJFlux.Short\",\n",
       "                         \"typeof(NNlib.softmax)\",\n",
       "                         \"Flux.Optimise.Adam\",\n",
       "                         \"typeof(Flux.Losses.crossentropy)\",\n",
       "                         \"Int64\",\n",
       "                         \"Int64\",\n",
       "                         \"Float64\",\n",
       "                         \"Float64\",\n",
       "                         \"Union{Int64, Random.AbstractRNG}\",\n",
       "                         \"Bool\",\n",
       "                         \"ComputationalResources.AbstractResource\"),\n",
       " hyperparameters = (:builder,\n",
       "                    :finaliser,\n",
       "                    :optimiser,\n",
       "                    :loss,\n",
       "                    :epochs,\n",
       "                    :batch_size,\n",
       "                    :lambda,\n",
       "                    :alpha,\n",
       "                    :rng,\n",
       "                    :optimiser_changes_trigger_retraining,\n",
       "                    :acceleration),\n",
       " implemented_methods = [],\n",
       " inverse_transform_scitype = Unknown,\n",
       " is_pure_julia = true,\n",
       " is_wrapper = false,\n",
       " iteration_parameter = :epochs,\n",
       " load_path = \"MLJFlux.NeuralNetworkClassifier\",\n",
       " package_license = \"MIT\",\n",
       " package_url = \"https://github.com/alan-turing-institute/MLJFlux.jl\",\n",
       " package_uuid = \"094fc8d1-fd35-5302-93ea-dabda2abf845\",\n",
       " predict_scitype =\n",
       "     AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:Finite},\n",
       " prediction_type = :probabilistic,\n",
       " reporting_operations = (),\n",
       " reports_feature_importances = false,\n",
       " supports_class_weights = false,\n",
       " supports_online = false,\n",
       " supports_training_losses = true,\n",
       " supports_weights = false,\n",
       " transform_scitype = Unknown,\n",
       " input_scitype = Table{<:AbstractVector{<:Continuous}},\n",
       " target_scitype = AbstractVector{<:Finite},\n",
       " output_scitype = Unknown)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model information\n",
    "info(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In MLJ a *model* is just a structure containing hyperparameters. A *model* does not store learned parameters and *models* are mutable. To store the learned parameters we use a *machine*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of epochs to train the model\n",
    "model.epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifying if the model is ready to be trained in 30 epochs\n",
    "NeuralNetworkClassifier(epochs = 30) == model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Machine trained 0 times; caches data\n",
       "  model: NeuralNetworkClassifier(builder = Short(n_hidden = 0, …), …)\n",
       "  args: \n",
       "    1:\tSource @496 ⏎ `Table{AbstractVector{Continuous}}`\n",
       "    2:\tSource @741 ⏎ `AbstractVector{Multiclass{3}}`\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates the object that will store the trained model (machine)\n",
    "mach = machine(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A machine (mach) stores *learned* parameters, among other things. We trained this machine on 70% of the data and evaluated it on 30% of the validation data. Let's start by dividing all row indexes into subsets of `train` and `test`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2, 3, 4, 5, 6, 7, 8, 9, 10  …  96, 97, 98, 99, 100, 101, 102, 103, 104, 105], [106, 107, 108, 109, 110, 111, 112, 113, 114, 115  …  141, 142, 143, 144, 145, 146, 147, 148, 149, 150])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the data into train and test\n",
    "train, test = partition(1:length(y), 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training machine(NeuralNetworkClassifier(builder = Short(n_hidden = 0, …), …), …).\n",
      "└ @ MLJBase C:\\Users\\drena\\.julia\\packages\\MLJBase\\Fl6Zc\\src\\machines.jl:498\n",
      "┌ Info: Loss is 1.095\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 1.063\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 1.05\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 0.9819\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 1.006\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 0.9776\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 0.9528\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 0.9442\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 0.9278\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 0.9523\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 0.9553\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 0.9539\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 0.9405\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 0.8962\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 0.896\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 0.8941\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 0.8821\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 0.8714\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 0.8552\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 0.9064\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 0.8121\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 0.8777\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 0.847\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 0.8251\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 0.8669\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 0.8672\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 0.8048\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 0.8523\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 0.8349\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 0.8075\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Machine trained 1 time; caches data\n",
       "  model: NeuralNetworkClassifier(builder = Short(n_hidden = 0, …), …)\n",
       "  args: \n",
       "    1:\tSource @496 ⏎ `Table{AbstractVector{Continuous}}`\n",
       "    2:\tSource @741 ⏎ `AbstractVector{Multiclass{3}}`\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "fit!(mach, rows = train, verbosity = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt32, Float64}:\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.184, Iris-versicolor=>0.412, Iris-virginica=>0.404)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.152, Iris-versicolor=>0.414, Iris-virginica=>0.433)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.626, Iris-versicolor=>0.264, Iris-virginica=>0.111)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.107, Iris-versicolor=>0.413, Iris-virginica=>0.48)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.0931, Iris-versicolor=>0.411, Iris-virginica=>0.496)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.622, Iris-versicolor=>0.267, Iris-virginica=>0.112)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.139, Iris-versicolor=>0.414, Iris-virginica=>0.447)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.159, Iris-versicolor=>0.414, Iris-virginica=>0.427)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.111, Iris-versicolor=>0.413, Iris-virginica=>0.476)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.19, Iris-versicolor=>0.411, Iris-virginica=>0.399)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.137, Iris-versicolor=>0.415, Iris-virginica=>0.448)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.166, Iris-versicolor=>0.413, Iris-virginica=>0.42)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.132, Iris-versicolor=>0.414, Iris-virginica=>0.454)\n",
       " ⋮\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.134, Iris-versicolor=>0.415, Iris-virginica=>0.451)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.658, Iris-versicolor=>0.247, Iris-virginica=>0.0953)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.119, Iris-versicolor=>0.414, Iris-virginica=>0.467)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.103, Iris-versicolor=>0.413, Iris-virginica=>0.484)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.125, Iris-versicolor=>0.414, Iris-virginica=>0.46)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.234, Iris-versicolor=>0.407, Iris-virginica=>0.359)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.109, Iris-versicolor=>0.413, Iris-virginica=>0.478)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.67, Iris-versicolor=>0.24, Iris-virginica=>0.0902)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.101, Iris-versicolor=>0.412, Iris-virginica=>0.487)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.128, Iris-versicolor=>0.415, Iris-virginica=>0.457)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.646, Iris-versicolor=>0.253, Iris-virginica=>0.101)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.664, Iris-versicolor=>0.243, Iris-virginica=>0.093)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predictions with the trained model using test subset\n",
    "yhat = predict(mach, rows = test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt32, Float64}:\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.184, Iris-versicolor=>0.412, Iris-virginica=>0.404)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.152, Iris-versicolor=>0.414, Iris-virginica=>0.433)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.626, Iris-versicolor=>0.264, Iris-virginica=>0.111)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.107, Iris-versicolor=>0.413, Iris-virginica=>0.48)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.0931, Iris-versicolor=>0.411, Iris-virginica=>0.496)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualizing some predictions\n",
    "yhat[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(chain = Chain(Chain(Dense(4 => 3, σ), Dropout(0.5), Dense(3 => 3)), softmax),)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model's structure\n",
    "fitted_params(mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(training_losses = [1.0313327149013345, 1.094678737222779, 1.0626054965567409, 1.0501224292859999, 0.9819390133537828, 1.005601038499376, 0.9776316248925471, 0.9527787802110075, 0.9442154619836155, 0.9278315420249625  …  0.8121147979586336, 0.8776642404476613, 0.847027531500855, 0.8250966272277187, 0.8668947367982454, 0.867235008134675, 0.804788052712021, 0.852347777049318, 0.8349146136226034, 0.8075178885822851],)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Error report throughout training\n",
    "report(mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6883513848429255"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculates the mean error\n",
    "erro_medio = cross_entropy(predict(mach, X), y) |> mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We modify the model's hyperparameter\n",
    "model.optimiser.eta = model.optimiser.eta * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Machine trained 0 times; caches data\n",
       "  model: NeuralNetworkClassifier(builder = Short(n_hidden = 0, …), …)\n",
       "  args: \n",
       "    1:\tSource @549 ⏎ `Table{AbstractVector{Continuous}}`\n",
       "    2:\tSource @854 ⏎ `AbstractVector{Multiclass{3}}`\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We recreate the machine\n",
    "mach = machine(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training machine(NeuralNetworkClassifier(builder = Short(n_hidden = 0, …), …), …).\n",
      "└ @ MLJBase C:\\Users\\drena\\.julia\\packages\\MLJBase\\Fl6Zc\\src\\machines.jl:498\n",
      "┌ Info: Loss is 1.295\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 1.23\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 1.157\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 1.18\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 1.132\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 1.1\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 1.096\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 1.095\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 1.052\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 1.047\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 1.042\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 1.042\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 1.005\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 1.001\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 0.9487\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 0.9723\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 0.9102\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 0.904\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 0.9214\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 0.8848\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 0.8484\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 0.8403\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 0.8846\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 0.8649\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 0.8173\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 0.7334\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 0.7564\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 0.7627\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 0.7371\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n",
      "┌ Info: Loss is 0.7584\n",
      "└ @ MLJFlux C:\\Users\\drena\\.julia\\packages\\MLJFlux\\jJ5DQ\\src\\core.jl:105\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Machine trained 1 time; caches data\n",
       "  model: NeuralNetworkClassifier(builder = Short(n_hidden = 0, …), …)\n",
       "  args: \n",
       "    1:\tSource @549 ⏎ `Table{AbstractVector{Continuous}}`\n",
       "    2:\tSource @854 ⏎ `AbstractVector{Multiclass{3}}`\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model again\n",
    "fit!(mach, rows = train, verbosity = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6140995090343341"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model's mean error\n",
    "erro_medio = cross_entropy(predict(mach, X), y) |> mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NumericRange(1 ≤ epochs ≤ 200; origin=100.5, unit=99.5; on log10 scale)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Range of values to create the learning curve\n",
    "r = range(model, :epochs, lower = 1, upper = 200, scale = :log10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training machine(ProbabilisticTunedModel(model = NeuralNetworkClassifier(builder = Short(n_hidden = 0, …), …), …), …).\n",
      "└ @ MLJBase C:\\Users\\drena\\.julia\\packages\\MLJBase\\Fl6Zc\\src\\machines.jl:498\n",
      "┌ Info: Attempting to evaluate 25 models.\n",
      "└ @ MLJTuning C:\\Users\\drena\\.julia\\packages\\MLJTuning\\DO54j\\src\\tuned_models.jl:727\n",
      "\u001b[33mEvaluating over 25 metamodels: 100%[=========================] Time: 0:00:05\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(parameter_name = \"epochs\",\n",
       " parameter_scale = :log10,\n",
       " parameter_values = [1, 2, 3, 4, 5, 6, 7, 9, 11, 13  …  39, 46, 56, 67, 80, 96, 116, 139, 167, 200],\n",
       " measurements = [1.0900563415506774, 1.0385147342100902, 0.9966562674507013, 0.9686860257039845, 0.939037045537272, 0.9074913010921409, 0.8700272687848801, 0.8219982611006716, 0.7733818191939951, 0.7352489622955913  …  0.5415189195603484, 0.5148272802697769, 0.47202355284249253, 0.4479616146521908, 0.4261517166853637, 0.3911977828082468, 0.37187454038167544, 0.3490800878265858, 0.32741204833775583, 0.33418226823909686],)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Learning curve throughout the training\n",
    "curva_aprendizado = learning_curve(model,\n",
    "                                   X,\n",
    "                                   y,\n",
    "                                   range = r,\n",
    "                                   resampling = Holdout(fraction_train = 0.7),\n",
    "                                   measure = cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling Plots [91a5bcdd-55d7-5caf-9e0b-520d859cae80]\n",
      "└ @ Base loading.jl:1423\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip330\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip330)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip331\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip330)\" d=\"\n",
       "M219.866 1410.9 L2352.76 1410.9 L2352.76 47.2441 L219.866 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip332\">\n",
       "    <rect x=\"219\" y=\"47\" width=\"2134\" height=\"1365\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip332)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  280.231,1410.9 280.231,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip332)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1154.69,1410.9 1154.69,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip332)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2029.15,1410.9 2029.15,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip330)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  219.866,1410.9 2352.76,1410.9 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip330)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  280.231,1410.9 280.231,1392 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip330)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1154.69,1410.9 1154.69,1392 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip330)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2029.15,1410.9 2029.15,1392 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip330)\" d=\"M245.214 1485.02 L252.853 1485.02 L252.853 1458.66 L244.543 1460.32 L244.543 1456.06 L252.806 1454.4 L257.482 1454.4 L257.482 1485.02 L265.121 1485.02 L265.121 1488.96 L245.214 1488.96 L245.214 1485.02 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M284.566 1457.48 Q280.954 1457.48 279.126 1461.04 Q277.32 1464.58 277.32 1471.71 Q277.32 1478.82 279.126 1482.38 Q280.954 1485.92 284.566 1485.92 Q288.2 1485.92 290.005 1482.38 Q291.834 1478.82 291.834 1471.71 Q291.834 1464.58 290.005 1461.04 Q288.2 1457.48 284.566 1457.48 M284.566 1453.77 Q290.376 1453.77 293.431 1458.38 Q296.51 1462.96 296.51 1471.71 Q296.51 1480.44 293.431 1485.04 Q290.376 1489.63 284.566 1489.63 Q278.755 1489.63 275.677 1485.04 Q272.621 1480.44 272.621 1471.71 Q272.621 1462.96 275.677 1458.38 Q278.755 1453.77 284.566 1453.77 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M306.215 1435.97 Q303.281 1435.97 301.795 1438.86 Q300.328 1441.74 300.328 1447.53 Q300.328 1453.31 301.795 1456.2 Q303.281 1459.08 306.215 1459.08 Q309.168 1459.08 310.635 1456.2 Q312.12 1453.31 312.12 1447.53 Q312.12 1441.74 310.635 1438.86 Q309.168 1435.97 306.215 1435.97 M306.215 1432.96 Q310.935 1432.96 313.418 1436.7 Q315.92 1440.43 315.92 1447.53 Q315.92 1454.62 313.418 1458.37 Q310.935 1462.09 306.215 1462.09 Q301.494 1462.09 298.993 1458.37 Q296.51 1454.62 296.51 1447.53 Q296.51 1440.43 298.993 1436.7 Q301.494 1432.96 306.215 1432.96 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M1121.02 1485.02 L1128.66 1485.02 L1128.66 1458.66 L1120.35 1460.32 L1120.35 1456.06 L1128.61 1454.4 L1133.29 1454.4 L1133.29 1485.02 L1140.93 1485.02 L1140.93 1488.96 L1121.02 1488.96 L1121.02 1485.02 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M1160.37 1457.48 Q1156.76 1457.48 1154.93 1461.04 Q1153.13 1464.58 1153.13 1471.71 Q1153.13 1478.82 1154.93 1482.38 Q1156.76 1485.92 1160.37 1485.92 Q1164.01 1485.92 1165.81 1482.38 Q1167.64 1478.82 1167.64 1471.71 Q1167.64 1464.58 1165.81 1461.04 Q1164.01 1457.48 1160.37 1457.48 M1160.37 1453.77 Q1166.18 1453.77 1169.24 1458.38 Q1172.32 1462.96 1172.32 1471.71 Q1172.32 1480.44 1169.24 1485.04 Q1166.18 1489.63 1160.37 1489.63 Q1154.56 1489.63 1151.48 1485.04 Q1148.43 1480.44 1148.43 1471.71 Q1148.43 1462.96 1151.48 1458.38 Q1154.56 1453.77 1160.37 1453.77 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M1172.86 1458.35 L1179.07 1458.35 L1179.07 1436.93 L1172.32 1438.28 L1172.32 1434.82 L1179.03 1433.47 L1182.83 1433.47 L1182.83 1458.35 L1189.04 1458.35 L1189.04 1461.55 L1172.86 1461.55 L1172.86 1458.35 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M1994.92 1485.02 L2002.56 1485.02 L2002.56 1458.66 L1994.25 1460.32 L1994.25 1456.06 L2002.52 1454.4 L2007.19 1454.4 L2007.19 1485.02 L2014.83 1485.02 L2014.83 1488.96 L1994.92 1488.96 L1994.92 1485.02 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M2034.28 1457.48 Q2030.67 1457.48 2028.84 1461.04 Q2027.03 1464.58 2027.03 1471.71 Q2027.03 1478.82 2028.84 1482.38 Q2030.67 1485.92 2034.28 1485.92 Q2037.91 1485.92 2039.72 1482.38 Q2041.55 1478.82 2041.55 1471.71 Q2041.55 1464.58 2039.72 1461.04 Q2037.91 1457.48 2034.28 1457.48 M2034.28 1453.77 Q2040.09 1453.77 2043.14 1458.38 Q2046.22 1462.96 2046.22 1471.71 Q2046.22 1480.44 2043.14 1485.04 Q2040.09 1489.63 2034.28 1489.63 Q2028.47 1489.63 2025.39 1485.04 Q2022.33 1480.44 2022.33 1471.71 Q2022.33 1462.96 2025.39 1458.38 Q2028.47 1453.77 2034.28 1453.77 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M2050.79 1458.35 L2064.05 1458.35 L2064.05 1461.55 L2046.22 1461.55 L2046.22 1458.35 Q2048.38 1456.11 2052.11 1452.35 Q2055.85 1448.57 2056.81 1447.48 Q2058.63 1445.43 2059.35 1444.02 Q2060.08 1442.59 2060.08 1441.22 Q2060.08 1438.98 2058.5 1437.57 Q2056.94 1436.16 2054.42 1436.16 Q2052.63 1436.16 2050.64 1436.78 Q2048.67 1437.4 2046.41 1438.66 L2046.41 1434.82 Q2048.7 1433.9 2050.7 1433.43 Q2052.69 1432.96 2054.35 1432.96 Q2058.71 1432.96 2061.3 1435.14 Q2063.9 1437.32 2063.9 1440.97 Q2063.9 1442.7 2063.24 1444.26 Q2062.6 1445.8 2060.89 1447.91 Q2060.42 1448.46 2057.9 1451.07 Q2055.38 1453.67 2050.79 1458.35 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M1206.5 1554.9 L1206.5 1557.76 L1179.57 1557.76 Q1179.96 1563.81 1183.2 1566.99 Q1186.48 1570.14 1192.31 1570.14 Q1195.68 1570.14 1198.83 1569.32 Q1202.01 1568.49 1205.13 1566.83 L1205.13 1572.37 Q1201.98 1573.71 1198.67 1574.41 Q1195.36 1575.11 1191.96 1575.11 Q1183.43 1575.11 1178.43 1570.14 Q1173.46 1565.18 1173.46 1556.71 Q1173.46 1547.96 1178.17 1542.83 Q1182.92 1537.68 1190.94 1537.68 Q1198.13 1537.68 1202.3 1542.33 Q1206.5 1546.94 1206.5 1554.9 M1200.64 1553.18 Q1200.58 1548.37 1197.94 1545.51 Q1195.33 1542.64 1191 1542.64 Q1186.1 1542.64 1183.14 1545.41 Q1180.21 1548.18 1179.77 1553.21 L1200.64 1553.18 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M1221.78 1568.84 L1221.78 1587.74 L1215.89 1587.74 L1215.89 1538.54 L1221.78 1538.54 L1221.78 1543.95 Q1223.62 1540.77 1226.43 1539.24 Q1229.26 1537.68 1233.17 1537.68 Q1239.67 1537.68 1243.71 1542.83 Q1247.78 1547.99 1247.78 1556.39 Q1247.78 1564.8 1243.71 1569.95 Q1239.67 1575.11 1233.17 1575.11 Q1229.26 1575.11 1226.43 1573.58 Q1223.62 1572.02 1221.78 1568.84 M1241.7 1556.39 Q1241.7 1549.93 1239.03 1546.27 Q1236.39 1542.58 1231.74 1542.58 Q1227.09 1542.58 1224.42 1546.27 Q1221.78 1549.93 1221.78 1556.39 Q1221.78 1562.85 1224.42 1566.55 Q1227.09 1570.21 1231.74 1570.21 Q1236.39 1570.21 1239.03 1566.55 Q1241.7 1562.85 1241.7 1556.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M1271.3 1542.64 Q1266.59 1542.64 1263.86 1546.34 Q1261.12 1550 1261.12 1556.39 Q1261.12 1562.79 1263.82 1566.48 Q1266.56 1570.14 1271.3 1570.14 Q1275.98 1570.14 1278.72 1566.45 Q1281.46 1562.76 1281.46 1556.39 Q1281.46 1550.06 1278.72 1546.37 Q1275.98 1542.64 1271.3 1542.64 M1271.3 1537.68 Q1278.94 1537.68 1283.3 1542.64 Q1287.66 1547.61 1287.66 1556.39 Q1287.66 1565.15 1283.3 1570.14 Q1278.94 1575.11 1271.3 1575.11 Q1263.63 1575.11 1259.27 1570.14 Q1254.94 1565.15 1254.94 1556.39 Q1254.94 1547.61 1259.27 1542.64 Q1263.63 1537.68 1271.3 1537.68 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M1323.03 1539.91 L1323.03 1545.38 Q1320.54 1544.01 1318.03 1543.34 Q1315.55 1542.64 1313 1542.64 Q1307.3 1542.64 1304.15 1546.27 Q1301 1549.87 1301 1556.39 Q1301 1562.92 1304.15 1566.55 Q1307.3 1570.14 1313 1570.14 Q1315.55 1570.14 1318.03 1569.47 Q1320.54 1568.77 1323.03 1567.41 L1323.03 1572.82 Q1320.57 1573.96 1317.93 1574.54 Q1315.32 1575.11 1312.36 1575.11 Q1304.31 1575.11 1299.57 1570.05 Q1294.83 1564.99 1294.83 1556.39 Q1294.83 1547.67 1299.6 1542.68 Q1304.41 1537.68 1312.74 1537.68 Q1315.45 1537.68 1318.03 1538.25 Q1320.61 1538.79 1323.03 1539.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M1362.84 1552.67 L1362.84 1574.19 L1356.99 1574.19 L1356.99 1552.86 Q1356.99 1547.8 1355.01 1545.29 Q1353.04 1542.77 1349.09 1542.77 Q1344.35 1542.77 1341.61 1545.79 Q1338.88 1548.82 1338.88 1554.04 L1338.88 1574.19 L1332.99 1574.19 L1332.99 1524.66 L1338.88 1524.66 L1338.88 1544.08 Q1340.98 1540.86 1343.81 1539.27 Q1346.67 1537.68 1350.4 1537.68 Q1356.54 1537.68 1359.69 1541.5 Q1362.84 1545.29 1362.84 1552.67 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M1397.25 1539.59 L1397.25 1545.13 Q1394.77 1543.85 1392.09 1543.22 Q1389.42 1542.58 1386.56 1542.58 Q1382.19 1542.58 1380 1543.92 Q1377.83 1545.25 1377.83 1547.93 Q1377.83 1549.96 1379.39 1551.14 Q1380.95 1552.29 1385.66 1553.34 L1387.67 1553.78 Q1393.91 1555.12 1396.52 1557.57 Q1399.16 1559.99 1399.16 1564.35 Q1399.16 1569.32 1395.21 1572.21 Q1391.3 1575.11 1384.42 1575.11 Q1381.56 1575.11 1378.44 1574.54 Q1375.35 1573.99 1371.91 1572.88 L1371.91 1566.83 Q1375.16 1568.52 1378.31 1569.38 Q1381.46 1570.21 1384.55 1570.21 Q1388.69 1570.21 1390.92 1568.81 Q1393.14 1567.37 1393.14 1564.8 Q1393.14 1562.41 1391.52 1561.14 Q1389.93 1559.86 1384.49 1558.68 L1382.45 1558.21 Q1377.01 1557.06 1374.59 1554.71 Q1372.17 1552.32 1372.17 1548.18 Q1372.17 1543.15 1375.73 1540.42 Q1379.3 1537.68 1385.85 1537.68 Q1389.1 1537.68 1391.97 1538.16 Q1394.83 1538.63 1397.25 1539.59 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip332)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  219.866,1249.86 2352.76,1249.86 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip332)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  219.866,912.488 2352.76,912.488 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip332)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  219.866,575.119 2352.76,575.119 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip332)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  219.866,237.749 2352.76,237.749 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip330)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  219.866,1410.9 219.866,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip330)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  219.866,1249.86 238.764,1249.86 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip330)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  219.866,912.488 238.764,912.488 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip330)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  219.866,575.119 238.764,575.119 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip330)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  219.866,237.749 238.764,237.749 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip330)\" d=\"M126.205 1235.66 Q122.593 1235.66 120.765 1239.22 Q118.959 1242.76 118.959 1249.89 Q118.959 1257 120.765 1260.56 Q122.593 1264.11 126.205 1264.11 Q129.839 1264.11 131.644 1260.56 Q133.473 1257 133.473 1249.89 Q133.473 1242.76 131.644 1239.22 Q129.839 1235.66 126.205 1235.66 M126.205 1231.95 Q132.015 1231.95 135.07 1236.56 Q138.149 1241.14 138.149 1249.89 Q138.149 1258.62 135.07 1263.23 Q132.015 1267.81 126.205 1267.81 Q120.394 1267.81 117.316 1263.23 Q114.26 1258.62 114.26 1249.89 Q114.26 1241.14 117.316 1236.56 Q120.394 1231.95 126.205 1231.95 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M146.366 1261.26 L151.251 1261.26 L151.251 1267.14 L146.366 1267.14 L146.366 1261.26 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M174.283 1236.65 L162.477 1255.1 L174.283 1255.1 L174.283 1236.65 M173.056 1232.58 L178.936 1232.58 L178.936 1255.1 L183.866 1255.1 L183.866 1258.99 L178.936 1258.99 L178.936 1267.14 L174.283 1267.14 L174.283 1258.99 L158.681 1258.99 L158.681 1254.48 L173.056 1232.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M126.529 898.287 Q122.918 898.287 121.089 901.852 Q119.283 905.394 119.283 912.523 Q119.283 919.63 121.089 923.194 Q122.918 926.736 126.529 926.736 Q130.163 926.736 131.968 923.194 Q133.797 919.63 133.797 912.523 Q133.797 905.394 131.968 901.852 Q130.163 898.287 126.529 898.287 M126.529 894.583 Q132.339 894.583 135.394 899.19 Q138.473 903.773 138.473 912.523 Q138.473 921.25 135.394 925.856 Q132.339 930.44 126.529 930.44 Q120.718 930.44 117.64 925.856 Q114.584 921.25 114.584 912.523 Q114.584 903.773 117.64 899.19 Q120.718 894.583 126.529 894.583 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M146.691 923.889 L151.575 923.889 L151.575 929.768 L146.691 929.768 L146.691 923.889 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M172.339 910.625 Q169.19 910.625 167.339 912.778 Q165.51 914.93 165.51 918.68 Q165.51 922.407 167.339 924.583 Q169.19 926.736 172.339 926.736 Q175.487 926.736 177.315 924.583 Q179.167 922.407 179.167 918.68 Q179.167 914.93 177.315 912.778 Q175.487 910.625 172.339 910.625 M181.621 895.972 L181.621 900.232 Q179.862 899.398 178.056 898.958 Q176.274 898.519 174.514 898.519 Q169.885 898.519 167.431 901.644 Q165.001 904.769 164.653 911.088 Q166.019 909.074 168.079 908.009 Q170.139 906.921 172.616 906.921 Q177.825 906.921 180.834 910.093 Q183.866 913.241 183.866 918.68 Q183.866 924.005 180.718 927.222 Q177.57 930.44 172.339 930.44 Q166.343 930.44 163.172 925.856 Q160.001 921.25 160.001 912.523 Q160.001 904.329 163.89 899.468 Q167.778 894.583 174.329 894.583 Q176.089 894.583 177.871 894.931 Q179.676 895.278 181.621 895.972 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M126.783 560.918 Q123.172 560.918 121.343 564.482 Q119.538 568.024 119.538 575.154 Q119.538 582.26 121.343 585.825 Q123.172 589.366 126.783 589.366 Q130.417 589.366 132.223 585.825 Q134.052 582.26 134.052 575.154 Q134.052 568.024 132.223 564.482 Q130.417 560.918 126.783 560.918 M126.783 557.214 Q132.593 557.214 135.649 561.82 Q138.728 566.404 138.728 575.154 Q138.728 583.88 135.649 588.487 Q132.593 593.07 126.783 593.07 Q120.973 593.07 117.894 588.487 Q114.839 583.88 114.839 575.154 Q114.839 566.404 117.894 561.82 Q120.973 557.214 126.783 557.214 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M146.945 586.519 L151.829 586.519 L151.829 592.399 L146.945 592.399 L146.945 586.519 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M172.014 575.987 Q168.681 575.987 166.76 577.769 Q164.862 579.552 164.862 582.677 Q164.862 585.802 166.76 587.584 Q168.681 589.366 172.014 589.366 Q175.348 589.366 177.269 587.584 Q179.19 585.779 179.19 582.677 Q179.19 579.552 177.269 577.769 Q175.371 575.987 172.014 575.987 M167.339 573.996 Q164.329 573.255 162.64 571.195 Q160.973 569.135 160.973 566.172 Q160.973 562.029 163.913 559.621 Q166.876 557.214 172.014 557.214 Q177.176 557.214 180.116 559.621 Q183.056 562.029 183.056 566.172 Q183.056 569.135 181.366 571.195 Q179.7 573.255 176.714 573.996 Q180.093 574.783 181.968 577.075 Q183.866 579.367 183.866 582.677 Q183.866 587.7 180.788 590.385 Q177.732 593.07 172.014 593.07 Q166.297 593.07 163.218 590.385 Q160.163 587.7 160.163 582.677 Q160.163 579.367 162.061 577.075 Q163.959 574.783 167.339 573.996 M165.626 566.612 Q165.626 569.297 167.292 570.802 Q168.982 572.306 172.014 572.306 Q175.024 572.306 176.714 570.802 Q178.426 569.297 178.426 566.612 Q178.426 563.927 176.714 562.422 Q175.024 560.918 172.014 560.918 Q168.982 560.918 167.292 562.422 Q165.626 563.927 165.626 566.612 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M117.501 251.094 L125.14 251.094 L125.14 224.729 L116.83 226.395 L116.83 222.136 L125.093 220.469 L129.769 220.469 L129.769 251.094 L137.408 251.094 L137.408 255.029 L117.501 255.029 L117.501 251.094 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M146.853 249.15 L151.737 249.15 L151.737 255.029 L146.853 255.029 L146.853 249.15 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M171.922 223.548 Q168.311 223.548 166.482 227.113 Q164.677 230.654 164.677 237.784 Q164.677 244.891 166.482 248.455 Q168.311 251.997 171.922 251.997 Q175.556 251.997 177.362 248.455 Q179.19 244.891 179.19 237.784 Q179.19 230.654 177.362 227.113 Q175.556 223.548 171.922 223.548 M171.922 219.844 Q177.732 219.844 180.788 224.451 Q183.866 229.034 183.866 237.784 Q183.866 246.511 180.788 251.117 Q177.732 255.701 171.922 255.701 Q166.112 255.701 163.033 251.117 Q159.978 246.511 159.978 237.784 Q159.978 229.034 163.033 224.451 Q166.112 219.844 171.922 219.844 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M16.4842 1228.95 L16.4842 1198.91 L21.895 1198.91 L21.895 1222.52 L35.9632 1222.52 L35.9632 1199.89 L41.3741 1199.89 L41.3741 1222.52 L58.5933 1222.52 L58.5933 1198.33 L64.0042 1198.33 L64.0042 1228.95 L16.4842 1228.95 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M33.8307 1167.37 Q33.2578 1168.35 33.0032 1169.53 Q32.7167 1170.68 32.7167 1172.08 Q32.7167 1177.04 35.9632 1179.71 Q39.1779 1182.36 45.2253 1182.36 L64.0042 1182.36 L64.0042 1188.24 L28.3562 1188.24 L28.3562 1182.36 L33.8944 1182.36 Q30.6479 1180.51 29.0883 1177.55 Q27.4968 1174.59 27.4968 1170.36 Q27.4968 1169.75 27.5923 1169.02 Q27.656 1168.29 27.8151 1167.4 L33.8307 1167.37 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M33.8307 1141.71 Q33.2578 1142.7 33.0032 1143.88 Q32.7167 1145.02 32.7167 1146.42 Q32.7167 1151.39 35.9632 1154.06 Q39.1779 1156.7 45.2253 1156.7 L64.0042 1156.7 L64.0042 1162.59 L28.3562 1162.59 L28.3562 1156.7 L33.8944 1156.7 Q30.6479 1154.86 29.0883 1151.9 Q27.4968 1148.94 27.4968 1144.7 Q27.4968 1144.1 27.5923 1143.37 Q27.656 1142.63 27.8151 1141.74 L33.8307 1141.71 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M32.4621 1123.19 Q32.4621 1127.9 36.1542 1130.64 Q39.8145 1133.37 46.212 1133.37 Q52.6095 1133.37 56.3017 1130.67 Q59.9619 1127.93 59.9619 1123.19 Q59.9619 1118.51 56.2698 1115.77 Q52.5777 1113.03 46.212 1113.03 Q39.8781 1113.03 36.186 1115.77 Q32.4621 1118.51 32.4621 1123.19 M27.4968 1123.19 Q27.4968 1115.55 32.4621 1111.19 Q37.4273 1106.83 46.212 1106.83 Q54.9649 1106.83 59.9619 1111.19 Q64.9272 1115.55 64.9272 1123.19 Q64.9272 1130.86 59.9619 1135.22 Q54.9649 1139.55 46.212 1139.55 Q37.4273 1139.55 32.4621 1135.22 Q27.4968 1130.86 27.4968 1123.19 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M41.3104 1069.72 L58.7206 1069.72 L58.7206 1059.4 Q58.7206 1054.21 56.5881 1051.73 Q54.4238 1049.22 49.9996 1049.22 Q45.5436 1049.22 43.4429 1051.73 Q41.3104 1054.21 41.3104 1059.4 L41.3104 1069.72 M21.7677 1069.72 L36.0905 1069.72 L36.0905 1060.2 Q36.0905 1055.49 34.34 1053.2 Q32.5576 1050.87 28.9291 1050.87 Q25.3325 1050.87 23.5501 1053.2 Q21.7677 1055.49 21.7677 1060.2 L21.7677 1069.72 M16.4842 1076.14 L16.4842 1059.72 Q16.4842 1052.37 19.5397 1048.39 Q22.5952 1044.41 28.2289 1044.41 Q32.5894 1044.41 35.1675 1046.45 Q37.7456 1048.49 38.3822 1052.43 Q39.4007 1047.69 42.6472 1045.08 Q45.8619 1042.44 50.6998 1042.44 Q57.0655 1042.44 60.5348 1046.77 Q64.0042 1051.1 64.0042 1059.08 L64.0042 1076.14 L16.4842 1076.14 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M46.0847 1015.48 Q46.0847 1022.58 47.7079 1025.31 Q49.3312 1028.05 53.2461 1028.05 Q56.3653 1028.05 58.2114 1026.01 Q60.0256 1023.95 60.0256 1020.41 Q60.0256 1015.54 56.5881 1012.61 Q53.1188 1009.65 47.3897 1009.65 L46.0847 1009.65 L46.0847 1015.48 M43.6657 1003.8 L64.0042 1003.8 L64.0042 1009.65 L58.5933 1009.65 Q61.8398 1011.66 63.3994 1014.65 Q64.9272 1017.64 64.9272 1021.97 Q64.9272 1027.45 61.8716 1030.69 Q58.7843 1033.91 53.6281 1033.91 Q47.6125 1033.91 44.5569 1029.9 Q41.5014 1025.86 41.5014 1017.87 L41.5014 1009.65 L40.9285 1009.65 Q36.8862 1009.65 34.6901 1012.33 Q32.4621 1014.97 32.4621 1019.78 Q32.4621 1022.83 33.1941 1025.73 Q33.9262 1028.62 35.3903 1031.3 L29.9795 1031.3 Q28.7381 1028.08 28.1334 1025.06 Q27.4968 1022.04 27.4968 1019.17 Q27.4968 1011.44 31.5072 1007.62 Q35.5176 1003.8 43.6657 1003.8 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M29.4065 969.01 L34.9447 969.01 Q33.6716 971.492 33.035 974.166 Q32.3984 976.84 32.3984 979.704 Q32.3984 984.065 33.7352 986.261 Q35.072 988.425 37.7456 988.425 Q39.7826 988.425 40.9603 986.866 Q42.1061 985.306 43.1565 980.595 L43.6021 978.59 Q44.9389 972.352 47.3897 969.742 Q49.8086 967.1 54.1691 967.1 Q59.1344 967.1 62.0308 971.047 Q64.9272 974.962 64.9272 981.837 Q64.9272 984.701 64.3543 987.82 Q63.8132 990.908 62.6992 994.345 L56.6518 994.345 Q58.3387 991.099 59.198 987.948 Q60.0256 984.797 60.0256 981.709 Q60.0256 977.572 58.6251 975.344 Q57.1929 973.116 54.6147 973.116 Q52.2276 973.116 50.9545 974.739 Q49.6813 976.33 48.5037 981.773 L48.0262 983.81 Q46.8804 989.253 44.5251 991.672 Q42.138 994.091 38.0002 994.091 Q32.9713 994.091 30.2341 990.526 Q27.4968 986.961 27.4968 980.404 Q27.4968 977.158 27.9743 974.293 Q28.4517 971.429 29.4065 969.01 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M44.7161 927.283 L47.5806 927.283 L47.5806 954.209 Q53.6281 953.828 56.8109 950.581 Q59.9619 947.303 59.9619 941.478 Q59.9619 938.104 59.1344 934.953 Q58.3069 931.77 56.6518 928.651 L62.1899 928.651 Q63.5267 931.802 64.227 935.112 Q64.9272 938.423 64.9272 941.828 Q64.9272 950.358 59.9619 955.355 Q54.9967 960.321 46.5303 960.321 Q37.7774 960.321 32.6531 955.61 Q27.4968 950.868 27.4968 942.847 Q27.4968 935.653 32.1438 931.484 Q36.7589 927.283 44.7161 927.283 M42.9973 933.139 Q38.1912 933.203 35.3266 935.844 Q32.4621 938.454 32.4621 942.783 Q32.4621 947.685 35.2312 950.645 Q38.0002 953.573 43.0292 954.019 L42.9973 933.139 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M46.0847 901.47 Q46.0847 908.567 47.7079 911.305 Q49.3312 914.042 53.2461 914.042 Q56.3653 914.042 58.2114 912.005 Q60.0256 909.936 60.0256 906.403 Q60.0256 901.533 56.5881 898.605 Q53.1188 895.645 47.3897 895.645 L46.0847 895.645 L46.0847 901.47 M43.6657 889.789 L64.0042 889.789 L64.0042 895.645 L58.5933 895.645 Q61.8398 897.65 63.3994 900.642 Q64.9272 903.634 64.9272 907.963 Q64.9272 913.437 61.8716 916.684 Q58.7843 919.898 53.6281 919.898 Q47.6125 919.898 44.5569 915.888 Q41.5014 911.846 41.5014 903.857 L41.5014 895.645 L40.9285 895.645 Q36.8862 895.645 34.6901 898.319 Q32.4621 900.96 32.4621 905.766 Q32.4621 908.822 33.1941 911.718 Q33.9262 914.615 35.3903 917.288 L29.9795 917.288 Q28.7381 914.074 28.1334 911.05 Q27.4968 908.026 27.4968 905.162 Q27.4968 897.427 31.5072 893.608 Q35.5176 889.789 43.6657 889.789 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M33.7671 854.268 L14.479 854.268 L14.479 848.411 L64.0042 848.411 L64.0042 854.268 L58.657 854.268 Q61.8398 856.114 63.3994 858.947 Q64.9272 861.748 64.9272 865.694 Q64.9272 872.156 59.771 876.23 Q54.6147 880.272 46.212 880.272 Q37.8093 880.272 32.6531 876.23 Q27.4968 872.156 27.4968 865.694 Q27.4968 861.748 29.0564 858.947 Q30.5842 856.114 33.7671 854.268 M46.212 874.224 Q52.6732 874.224 56.3653 871.583 Q60.0256 868.909 60.0256 864.262 Q60.0256 859.615 56.3653 856.942 Q52.6732 854.268 46.212 854.268 Q39.7508 854.268 36.0905 856.942 Q32.3984 859.615 32.3984 864.262 Q32.3984 868.909 36.0905 871.583 Q39.7508 874.224 46.212 874.224 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M32.4621 822.535 Q32.4621 827.245 36.1542 829.983 Q39.8145 832.72 46.212 832.72 Q52.6095 832.72 56.3017 830.015 Q59.9619 827.277 59.9619 822.535 Q59.9619 817.856 56.2698 815.119 Q52.5777 812.382 46.212 812.382 Q39.8781 812.382 36.186 815.119 Q32.4621 817.856 32.4621 822.535 M27.4968 822.535 Q27.4968 814.896 32.4621 810.535 Q37.4273 806.175 46.212 806.175 Q54.9649 806.175 59.9619 810.535 Q64.9272 814.896 64.9272 822.535 Q64.9272 830.206 59.9619 834.566 Q54.9649 838.895 46.212 838.895 Q37.4273 838.895 32.4621 834.566 Q27.4968 830.206 27.4968 822.535 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M42.4881 746.115 L64.0042 746.115 L64.0042 751.971 L42.679 751.971 Q37.6183 751.971 35.1038 753.944 Q32.5894 755.918 32.5894 759.864 Q32.5894 764.607 35.6131 767.344 Q38.6368 770.081 43.8567 770.081 L64.0042 770.081 L64.0042 775.97 L28.3562 775.97 L28.3562 770.081 L33.8944 770.081 Q30.6797 767.981 29.0883 765.148 Q27.4968 762.283 27.4968 758.56 Q27.4968 752.417 31.3163 749.266 Q35.1038 746.115 42.4881 746.115 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M46.0847 718.233 Q46.0847 725.331 47.7079 728.068 Q49.3312 730.805 53.2461 730.805 Q56.3653 730.805 58.2114 728.768 Q60.0256 726.699 60.0256 723.166 Q60.0256 718.296 56.5881 715.368 Q53.1188 712.408 47.3897 712.408 L46.0847 712.408 L46.0847 718.233 M43.6657 706.552 L64.0042 706.552 L64.0042 712.408 L58.5933 712.408 Q61.8398 714.413 63.3994 717.405 Q64.9272 720.397 64.9272 724.726 Q64.9272 730.2 61.8716 733.447 Q58.7843 736.661 53.6281 736.661 Q47.6125 736.661 44.5569 732.651 Q41.5014 728.609 41.5014 720.62 L41.5014 712.408 L40.9285 712.408 Q36.8862 712.408 34.6901 715.082 Q32.4621 717.723 32.4621 722.53 Q32.4621 725.585 33.1941 728.482 Q33.9262 731.378 35.3903 734.052 L29.9795 734.052 Q28.7381 730.837 28.1334 727.813 Q27.4968 724.789 27.4968 721.925 Q27.4968 714.191 31.5072 710.371 Q35.5176 706.552 43.6657 706.552 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M20.1444 637.929 L26.9239 637.929 Q23.9002 641.176 22.4043 644.868 Q20.9083 648.528 20.9083 652.666 Q20.9083 660.814 25.9054 665.143 Q30.8707 669.471 40.2919 669.471 Q49.6813 669.471 54.6784 665.143 Q59.6436 660.814 59.6436 652.666 Q59.6436 648.528 58.1477 644.868 Q56.6518 641.176 53.6281 637.929 L60.3439 637.929 Q62.6355 641.303 63.7814 645.091 Q64.9272 648.847 64.9272 653.048 Q64.9272 663.838 58.3387 670.044 Q51.7183 676.251 40.2919 676.251 Q28.8336 676.251 22.2451 670.044 Q15.6248 663.838 15.6248 653.048 Q15.6248 648.783 16.7706 645.027 Q17.8846 641.24 20.1444 637.929 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M33.8307 607.597 Q33.2578 608.583 33.0032 609.761 Q32.7167 610.907 32.7167 612.307 Q32.7167 617.273 35.9632 619.946 Q39.1779 622.588 45.2253 622.588 L64.0042 622.588 L64.0042 628.476 L28.3562 628.476 L28.3562 622.588 L33.8944 622.588 Q30.6479 620.742 29.0883 617.782 Q27.4968 614.822 27.4968 610.589 Q27.4968 609.984 27.5923 609.252 Q27.656 608.52 27.8151 607.629 L33.8307 607.597 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M32.4621 589.073 Q32.4621 593.783 36.1542 596.52 Q39.8145 599.258 46.212 599.258 Q52.6095 599.258 56.3017 596.552 Q59.9619 593.815 59.9619 589.073 Q59.9619 584.394 56.2698 581.657 Q52.5777 578.919 46.212 578.919 Q39.8781 578.919 36.186 581.657 Q32.4621 584.394 32.4621 589.073 M27.4968 589.073 Q27.4968 581.434 32.4621 577.073 Q37.4273 572.713 46.212 572.713 Q54.9649 572.713 59.9619 577.073 Q64.9272 581.434 64.9272 589.073 Q64.9272 596.743 59.9619 601.104 Q54.9649 605.432 46.212 605.432 Q37.4273 605.432 32.4621 601.104 Q27.4968 596.743 27.4968 589.073 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M29.4065 540.279 L34.9447 540.279 Q33.6716 542.762 33.035 545.436 Q32.3984 548.109 32.3984 550.974 Q32.3984 555.334 33.7352 557.53 Q35.072 559.695 37.7456 559.695 Q39.7826 559.695 40.9603 558.135 Q42.1061 556.576 43.1565 551.865 L43.6021 549.86 Q44.9389 543.621 47.3897 541.011 Q49.8086 538.37 54.1691 538.37 Q59.1344 538.37 62.0308 542.316 Q64.9272 546.231 64.9272 553.106 Q64.9272 555.971 64.3543 559.09 Q63.8132 562.177 62.6992 565.615 L56.6518 565.615 Q58.3387 562.368 59.198 559.217 Q60.0256 556.066 60.0256 552.979 Q60.0256 548.841 58.6251 546.613 Q57.1929 544.385 54.6147 544.385 Q52.2276 544.385 50.9545 546.009 Q49.6813 547.6 48.5037 553.043 L48.0262 555.08 Q46.8804 560.522 44.5251 562.941 Q42.138 565.36 38.0002 565.36 Q32.9713 565.36 30.2341 561.796 Q27.4968 558.231 27.4968 551.674 Q27.4968 548.428 27.9743 545.563 Q28.4517 542.698 29.4065 540.279 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M29.4065 506.318 L34.9447 506.318 Q33.6716 508.801 33.035 511.475 Q32.3984 514.148 32.3984 517.013 Q32.3984 521.373 33.7352 523.569 Q35.072 525.734 37.7456 525.734 Q39.7826 525.734 40.9603 524.174 Q42.1061 522.615 43.1565 517.904 L43.6021 515.899 Q44.9389 509.66 47.3897 507.05 Q49.8086 504.409 54.1691 504.409 Q59.1344 504.409 62.0308 508.355 Q64.9272 512.27 64.9272 519.145 Q64.9272 522.01 64.3543 525.129 Q63.8132 528.216 62.6992 531.654 L56.6518 531.654 Q58.3387 528.407 59.198 525.256 Q60.0256 522.105 60.0256 519.018 Q60.0256 514.88 58.6251 512.652 Q57.1929 510.424 54.6147 510.424 Q52.2276 510.424 50.9545 512.048 Q49.6813 513.639 48.5037 519.082 L48.0262 521.119 Q46.8804 526.561 44.5251 528.98 Q42.138 531.399 38.0002 531.399 Q32.9713 531.399 30.2341 527.834 Q27.4968 524.27 27.4968 517.713 Q27.4968 514.466 27.9743 511.602 Q28.4517 508.737 29.4065 506.318 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M16.4842 474.108 L16.4842 444.062 L21.895 444.062 L21.895 467.679 L35.9632 467.679 L35.9632 445.048 L41.3741 445.048 L41.3741 467.679 L58.5933 467.679 L58.5933 443.489 L64.0042 443.489 L64.0042 474.108 L16.4842 474.108 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M42.4881 403.544 L64.0042 403.544 L64.0042 409.4 L42.679 409.4 Q37.6183 409.4 35.1038 411.374 Q32.5894 413.347 32.5894 417.294 Q32.5894 422.036 35.6131 424.774 Q38.6368 427.511 43.8567 427.511 L64.0042 427.511 L64.0042 433.399 L28.3562 433.399 L28.3562 427.511 L33.8944 427.511 Q30.6797 425.41 29.0883 422.578 Q27.4968 419.713 27.4968 415.989 Q27.4968 409.846 31.3163 406.695 Q35.1038 403.544 42.4881 403.544 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M18.2347 386.07 L28.3562 386.07 L28.3562 374.007 L32.9077 374.007 L32.9077 386.07 L52.2594 386.07 Q56.6199 386.07 57.8613 384.893 Q59.1026 383.683 59.1026 380.023 L59.1026 374.007 L64.0042 374.007 L64.0042 380.023 Q64.0042 386.802 61.4897 389.38 Q58.9434 391.958 52.2594 391.958 L32.9077 391.958 L32.9077 396.255 L28.3562 396.255 L28.3562 391.958 L18.2347 391.958 L18.2347 386.07 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M33.8307 345.648 Q33.2578 346.635 33.0032 347.812 Q32.7167 348.958 32.7167 350.359 Q32.7167 355.324 35.9632 357.997 Q39.1779 360.639 45.2253 360.639 L64.0042 360.639 L64.0042 366.527 L28.3562 366.527 L28.3562 360.639 L33.8944 360.639 Q30.6479 358.793 29.0883 355.833 Q27.4968 352.873 27.4968 348.64 Q27.4968 348.035 27.5923 347.303 Q27.656 346.571 27.8151 345.68 L33.8307 345.648 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M32.4621 327.124 Q32.4621 331.834 36.1542 334.572 Q39.8145 337.309 46.212 337.309 Q52.6095 337.309 56.3017 334.603 Q59.9619 331.866 59.9619 327.124 Q59.9619 322.445 56.2698 319.708 Q52.5777 316.97 46.212 316.97 Q39.8781 316.97 36.186 319.708 Q32.4621 322.445 32.4621 327.124 M27.4968 327.124 Q27.4968 319.485 32.4621 315.124 Q37.4273 310.764 46.212 310.764 Q54.9649 310.764 59.9619 315.124 Q64.9272 319.485 64.9272 327.124 Q64.9272 334.794 59.9619 339.155 Q54.9649 343.484 46.212 343.484 Q37.4273 343.484 32.4621 339.155 Q27.4968 334.794 27.4968 327.124 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M58.657 295.391 L77.5631 295.391 L77.5631 301.279 L28.3562 301.279 L28.3562 295.391 L33.7671 295.391 Q30.5842 293.545 29.0564 290.744 Q27.4968 287.911 27.4968 283.996 Q27.4968 277.503 32.6531 273.461 Q37.8093 269.387 46.212 269.387 Q54.6147 269.387 59.771 273.461 Q64.9272 277.503 64.9272 283.996 Q64.9272 287.911 63.3994 290.744 Q61.8398 293.545 58.657 295.391 M46.212 275.466 Q39.7508 275.466 36.0905 278.14 Q32.3984 280.781 32.3984 285.428 Q32.3984 290.075 36.0905 292.749 Q39.7508 295.391 46.212 295.391 Q52.6732 295.391 56.3653 292.749 Q60.0256 290.075 60.0256 285.428 Q60.0256 280.781 56.3653 278.14 Q52.6732 275.466 46.212 275.466 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M67.3143 244.847 Q73.68 247.33 75.6216 249.685 Q77.5631 252.04 77.5631 255.987 L77.5631 260.666 L72.6615 260.666 L72.6615 257.228 Q72.6615 254.809 71.5157 253.473 Q70.3699 252.136 66.1048 250.512 L63.4312 249.462 L28.3562 263.88 L28.3562 257.674 L56.238 246.534 L28.3562 235.394 L28.3562 229.187 L67.3143 244.847 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip332)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  280.231,85.838 543.47,172.781 697.455,243.39 806.709,290.571 891.453,340.585 960.694,393.797 1019.24,456.994 1114.68,538.011 1190.89,620.02 1254.33,684.344 \n",
       "  1308.68,740.585 1398.45,817.981 1454.13,867.877 1531.9,924.085 1596.43,970.426 1671.55,1011.14 1734.25,1056.16 1808.95,1128.37 1877.06,1168.95 1944.41,1205.74 \n",
       "  2013.65,1264.71 2085.52,1297.3 2154.21,1335.75 2223.91,1372.3 2292.39,1360.88 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip330)\" d=\"\n",
       "M1992.24 196.379 L2281.66 196.379 L2281.66 92.6992 L1992.24 92.6992  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip330)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1992.24,196.379 2281.66,196.379 2281.66,92.6992 1992.24,92.6992 1992.24,196.379 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip330)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2015.94,144.539 2158.13,144.539 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip330)\" d=\"M2195.67 164.227 Q2193.87 168.856 2192.16 170.268 Q2190.44 171.68 2187.57 171.68 L2184.17 171.68 L2184.17 168.115 L2186.67 168.115 Q2188.43 168.115 2189.4 167.282 Q2190.37 166.449 2191.55 163.347 L2192.32 161.403 L2181.83 135.893 L2186.35 135.893 L2194.45 156.171 L2202.55 135.893 L2207.06 135.893 L2195.67 164.227 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M2214.35 157.884 L2221.99 157.884 L2221.99 131.518 L2213.68 133.185 L2213.68 128.926 L2221.95 127.259 L2226.62 127.259 L2226.62 157.884 L2234.26 157.884 L2234.26 161.819 L2214.35 161.819 L2214.35 157.884 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plotting the learning curve\n",
    "using Plots\n",
    "plot(curva_aprendizado.parameter_values,\n",
    "     curva_aprendizado.measurements,\n",
    "     xlab = curva_aprendizado.parameter_name,\n",
    "     xscale = curva_aprendizado.parameter_scale,\n",
    "     ylab = \"Erro Baseado na Cross Entropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model into the disk\n",
    "MLJ.save(\"modelo/modelo_rede_neural_DRGM.jlso\", mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Machine trained 1 time; caches data\n",
       "  model: NeuralNetworkClassifier(builder = Short(n_hidden = 0, …), …)\n",
       "  args: \n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the model from the disk\n",
    "mach2 = machine(\"modelo/modelo_rede_neural_DRGM.jlso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt32, Float64}:\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.188, Iris-versicolor=>0.417, Iris-virginica=>0.394)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.153, Iris-versicolor=>0.403, Iris-virginica=>0.444)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.746, Iris-versicolor=>0.218, Iris-virginica=>0.0364)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.122, Iris-versicolor=>0.381, Iris-virginica=>0.497)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.0999, Iris-versicolor=>0.367, Iris-virginica=>0.533)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.707, Iris-versicolor=>0.247, Iris-virginica=>0.0459)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.142, Iris-versicolor=>0.399, Iris-virginica=>0.46)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.175, Iris-versicolor=>0.411, Iris-virginica=>0.414)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.129, Iris-versicolor=>0.386, Iris-virginica=>0.485)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.201, Iris-versicolor=>0.419, Iris-virginica=>0.381)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.139, Iris-versicolor=>0.397, Iris-virginica=>0.464)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.171, Iris-versicolor=>0.411, Iris-virginica=>0.418)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.129, Iris-versicolor=>0.392, Iris-virginica=>0.479)\n",
       " ⋮\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.132, Iris-versicolor=>0.394, Iris-virginica=>0.474)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.767, Iris-versicolor=>0.203, Iris-virginica=>0.0298)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.124, Iris-versicolor=>0.387, Iris-virginica=>0.488)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.107, Iris-versicolor=>0.374, Iris-virginica=>0.518)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.124, Iris-versicolor=>0.388, Iris-virginica=>0.489)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.232, Iris-versicolor=>0.428, Iris-virginica=>0.34)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.123, Iris-versicolor=>0.383, Iris-virginica=>0.493)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.773, Iris-versicolor=>0.199, Iris-virginica=>0.0284)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.112, Iris-versicolor=>0.375, Iris-virginica=>0.513)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.132, Iris-versicolor=>0.393, Iris-virginica=>0.475)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.774, Iris-versicolor=>0.197, Iris-virginica=>0.0293)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.779, Iris-versicolor=>0.194, Iris-virginica=>0.0274)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making predictions with test subset\n",
    "yhat = predict(mach2, X[test,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt32, Float64}:\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.188, Iris-versicolor=>0.417, Iris-virginica=>0.394)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.153, Iris-versicolor=>0.403, Iris-virginica=>0.444)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.746, Iris-versicolor=>0.218, Iris-virginica=>0.0364)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.122, Iris-versicolor=>0.381, Iris-virginica=>0.497)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.0999, Iris-versicolor=>0.367, Iris-virginica=>0.533)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualizing some predictions\n",
    "yhat[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39414086523896735"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the probability of \"Iris-virginica\" in the first prediction instance\n",
    "pdf(yhat[1], \"Iris-virginica\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45×3 Matrix{Float64}:\n",
       " 0.188497   0.417362  0.394141\n",
       " 0.152717   0.403227  0.444056\n",
       " 0.74596    0.217685  0.0363556\n",
       " 0.121749   0.381033  0.497217\n",
       " 0.0999266  0.36737   0.532704\n",
       " 0.707206   0.246884  0.0459106\n",
       " 0.141579   0.39865   0.459771\n",
       " 0.175174   0.410808  0.414018\n",
       " 0.129255   0.386178  0.484567\n",
       " 0.200682   0.418711  0.380607\n",
       " 0.1389     0.397375  0.463725\n",
       " 0.171407   0.410537  0.418057\n",
       " 0.128631   0.391872  0.479497\n",
       " ⋮                    \n",
       " 0.131886   0.393974  0.47414\n",
       " 0.767213   0.202939  0.0298488\n",
       " 0.124448   0.387154  0.488398\n",
       " 0.107414   0.374164  0.518422\n",
       " 0.123859   0.387569  0.488571\n",
       " 0.232231   0.428247  0.339522\n",
       " 0.123058   0.383467  0.493476\n",
       " 0.772619   0.198974  0.0284067\n",
       " 0.112214   0.375169  0.512617\n",
       " 0.132004   0.39319   0.474806\n",
       " 0.77352    0.197148  0.0293324\n",
       " 0.778813   0.193747  0.0274402"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the estimates of each row with the probabilities for each of the 3 classes\n",
    "L = levels(y)\n",
    "pdf(yhat, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 3
}
