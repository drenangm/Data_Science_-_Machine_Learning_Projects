# Construindo uma Rede Neural com Programa√ß√£o e Matem√°tica

A Arquitetura de Redes Neurais Artificiais
Uma rede neural t√≠pica √© constitu√≠da por um conjunto de neur√¥nios interligados, infuenciando uns aos outros formando um sistema maior, capaz de armazenar conhecimento adquirido por meio de exemplos apresentados e, assim, podendo realizar infer√™ncias sobre novos conjuntos de dados. Vejamos a arquitetura de redes neurais artificiais.

As redes neurais s√£o comumente apresentadas como um grafo orientado, onde os v√©rtices s√£o os neur√¥nios e as arestas as sinapses. A dire√ß√£o das arestas informa o tipo de alimenta√ß√£o, ou seja, como os neur√¥nios s√£o alimentados (recebem sinais de entrada). As redes neurais derivam seu poder devido a sua estrutura massiva e paralela e a habilidade de aprender por experi√™ncia. Essa experi√™ncia √© transmitida por meio de exemplos obtidos do mundo real, definidos como um conjunto de caracter√≠sticas formados por dados de entrada e de sa√≠da. Se apresentamos esses dados de entrada e sa√≠da √† rede, estamos diante de aprendizagem supervsionada e caso apresentemos apenas os dados de entrada, estamos diante de aprendizagem n√£o supervisionada!

O conhecimento obtido pela rede atrav√©s dos exemplos √© armazenado na forma de pesos das conex√µes, os quais ser√£o ajustados a fim de tomar decis√µes corretas a partir de novas entradas, ou seja, novas situa√ß√µes do mundo real n√£o conhecidas pela rede. O processo de ajuste dos pesos sinapticos √© realizado pelo algoritmo de aprendizagem, respons√°vel em armazenar na rede o conhecimento do mundo real obtido atraves de exemplos. Existem v√°rios algoritmos de aprendizagem, dentre eles o backpropagation que √© o algoritmo mais utilizado.

![image](https://user-images.githubusercontent.com/79231882/202472890-b6a80f91-de07-487b-89a2-ccaacf777671.png)


# Parte 1 - Implementando Uma Rede Neural Artificial Somente com F√≥rmulas Matem√°ticas (Sem Frameworks)


# Parte 1A - Forward Propagation
https://arxiv.org/pdf/1905.07490.pdf

![image](https://user-images.githubusercontent.com/79231882/202473110-e6be5049-c252-47d1-b11a-f17ef88668d1.png)


# Desenvolvendo a Fun√ß√£o Sigm√≥ide
A principal raz√£o pela qual usamos a fun√ß√£o sigm√≥ide √© porque ela permite converter n√∫meros para valores entre 0 e 1.

Portanto, √© especialmente usada para modelos em que temos que prever a probabilidade como uma sa√≠da. Como a probabilidade de qualquer coisa existir apenas entre o intervalo de 0 e 1, sigmoide √© a escolha certa. Algumas caracter√≠siticas da fun√ß√£o sigm√≥ide:

A fun√ß√£o √© diferenci√°vel. Isso significa que podemos encontrar a inclina√ß√£o da curva sigm√≥ide em dois pontos.
A fun√ß√£o sigm√≥ide log√≠stica pode fazer com que uma rede neural fique presa no momento do treinamento.
A fun√ß√£o softmax √© uma fun√ß√£o de ativa√ß√£o log√≠stica mais generalizada, utilizada para a classifica√ß√£o em v√°rias classes.

![image](https://user-images.githubusercontent.com/79231882/202473234-b278e7ea-930d-47cf-87a5-6f77324c7c23.png)

Se a fun√ß√£o parecer muito abstrata ou estranha para voc√™, n√£o se preocupe muito com detalhes como o n√∫mero de Euler e ou como algu√©m criou essa fun√ß√£o. Para aqueles que n√£o s√£o conhecedores de matem√°tica, a √∫nica coisa importante sobre a fun√ß√£o sigm√≥ide √© primeiro, sua curva e, segundo, sua derivada. Aqui est√£o mais alguns detalhes:

A fun√ß√£o sigm√≥ide produz resultados semelhantes aos da fun√ß√£o de passo (Step Function) em que a sa√≠da est√° entre 0 e 1. A curva cruza 0,5 a z = 0, e podemos definir regras para a fun√ß√£o de ativa√ß√£o, como: Se a sa√≠da do neur√¥nio sigm√≥ide for maior que ou igual a 0,5, gera 1; se a sa√≠da for menor que 0,5, gera 0.
A fun√ß√£o sigm√≥ide √© suave e possui uma derivada simples de œÉ(z) * (1 - œÉ (z)), que √© diferenci√°vel em qualquer lugar da curva.
Se z for muito negativo, a sa√≠da ser√° aproximadamente 0; se z for muito positivo, a sa√≠da √© aproximadamente 1; mas em torno de z = 0, onde z n√£o √© muito grande nem muito pequeno, temos um desvio relativamente maior √† medida que z muda.

![image](https://user-images.githubusercontent.com/79231882/202473318-8870a89e-b7c4-4c00-9bc6-6a0e0f8dd7f6.png)

No C√°lculo, a derivada em um ponto de uma fun√ß√£o y = f(x) representa a taxa de varia√ß√£o instant√¢nea de y em rela√ß√£o a x neste ponto.

Um exemplo t√≠pico √© a fun√ß√£o velocidade que representa a taxa de varia√ß√£o (derivada) da fun√ß√£o espa√ßo. Do mesmo modo, a fun√ß√£o acelera√ß√£o √© a derivada da fun√ß√£o velocidade. Geometricamente, a derivada no ponto x = a de y = f(x) representa a inclina√ß√£o da reta tangente ao gr√°fico desta fun√ß√£o no ponto (a, f(a)).

A fun√ß√£o que a cada ponto x associa a derivada neste ponto de f(x) √© chamada de fun√ß√£o derivada de f(x).

![image](https://user-images.githubusercontent.com/79231882/202473401-e40bc3fe-7b14-483f-9fb2-cc98d663e1ab.png)

Em cada ponto, a derivada de f(x) √© a tangente do √¢ngulo que a reta tangente √† curva faz em rela√ß√£o ao eixo das abscissas. A reta √© sempre tangente √† curva azul; a tangente do √¢ngulo que ela faz com o eixo das abscissas √© a derivada. Note-se que a derivada √© positiva quando verde, negativa quando vermelha, e zero quando preta.

A derivada de uma fun√ß√£o y = f(x) num ponto x = x0, √© igual ao valor da tangente trigonom√©trica do √¢ngulo formado pela tangente geom√©trica √† curva representativa de y=f(x), no ponto x = x0, ou seja, a derivada √© o coeficiente angular da reta tangente ao gr√°fico da fun√ß√£o no ponto x0.

A fun√ß√£o derivada √© representada por f'(x).

# Desenvolvendo a Fun√ß√£o ReLU

Para usar a descida de gradiente estoc√°stico com retropropaga√ß√£o de erros para treinar redes neurais profundas, √© necess√°ria uma fun√ß√£o de ativa√ß√£o que se assemelhe e atue como uma fun√ß√£o linear, mas √©, de fato, uma fun√ß√£o n√£o linear que permite que relacionamentos complexos nos dados sejam aprendidos.

A solu√ß√£o √© usar a fun√ß√£o de ativa√ß√£o linear retificada ou ReL para abreviar. Um n√≥ ou unidade que implementa essa fun√ß√£o de ativa√ß√£o √© chamado de unidade de ativa√ß√£o linear retificada ou ReLU, para abreviar. Frequentemente, as redes que usam a fun√ß√£o retificadora para as camadas ocultas s√£o chamadas de redes retificadas.

A fun√ß√£o ReLU √© definida como ùëì(ùë•) = max (0, ùë•). Normalmente, ela √© aplicada elemento a elemento √† sa√≠da de alguma outra fun√ß√£o, como um produto de vetor e matriz.

A ado√ß√£o da ReLU pode ser facilmente considerada um dos marcos na revolu√ß√£o do aprendizado profundo, por ex. as t√©cnicas que agora permitem o desenvolvimento rotineiro de redes neurais muito profundas.

A derivada da fun√ß√£o linear retificada tamb√©m √© f√°cil de calcular. A derivada da fun√ß√£o de ativa√ß√£o √© necess√°ria ao atualizar os pesos de um n√≥ como parte da retropropaga√ß√£o de erro.

A derivada da fun√ß√£o √© a inclina√ß√£o. A inclina√ß√£o para valores negativos √© 0,0 e a inclina√ß√£o para valores positivos √© 1,0.

Tradicionalmente, o campo das redes neurais evitou qualquer fun√ß√£o de ativa√ß√£o que n√£o fosse completamente diferenci√°vel, talvez adiando a ado√ß√£o da fun√ß√£o linear retificada e de outras fun√ß√µes lineares. Tecnicamente, n√£o podemos calcular a derivada quando a entrada √© 0,0; portanto, podemos assumir que √© zero. Este n√£o √© um problema na pr√°tica.

Os gradientes das ativa√ß√µes tangentes e hiperb√≥licas s√£o menores que a por√ß√£o positiva da ReLU. Isso significa que a parte positiva √© atualizada mais rapidamente √† medida que o treinamento avan√ßa. No entanto, isso tem um custo. O gradiente 0 no lado esquerdo tem seu pr√≥prio problema, chamado "neur√¥nios mortos", no qual uma atualiza√ß√£o de gradiente define os valores recebidos para uma ReLU, de modo que a sa√≠da √© sempre zero; unidades ReLU modificadas, como ELU (ou Leaky ReLU, ou PReLU, etc.) podem melhorar isso.

![image](https://user-images.githubusercontent.com/79231882/202473671-8e128a2b-f688-4a0f-a58e-215cd089b356.png)


# Funcao de ativacao ReLu

![image](https://user-images.githubusercontent.com/79231882/202473720-727f2f2d-2f65-4864-a975-795802f78598.png)


# Desenvolvendo a funcao de custo

![image](https://user-images.githubusercontent.com/79231882/202473957-557d0858-4e18-41d6-bd8c-b85fc52eed76.png)


# Parte 1B - Backward Propagation

![image](https://user-images.githubusercontent.com/79231882/202474067-9361d4a4-f508-4bad-afed-8ce3d1e78569.png)

